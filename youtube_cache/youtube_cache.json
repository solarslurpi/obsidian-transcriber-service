{"metadata": {"youtube_url": "https://www.youtube.com/watch?v=fb7LENb9eag", "title": "BERTopic Explained", "audio_quality": "tiny", "tags": "Huggingface, Tensorflow, artificial_intelligence, bert, bert_model_explained, bert_sentiment_analysis, freecodecamp, hdbscan, james_briggs, lda_topic_modelling_explained, machine_learning, natural_language_processing, nlp, nlproc, python, top2vec, topic_modeling, topic_modeling_python, topic_modelling, topic_modelling_nlp, umap", "description": "90% of the world's data is unstructured. It is built by humans, for humans. That's great for human consumption, but it is *very* hard to organize when we begin dealing with the massive amounts of data abundant in today's information age.\n\nOrganization is complicated because unstructured text data is not intended to be understood by machines, and having humans process this abundance of data is wildly expensive and *very slow*.\n\nFortunately, there is light at the end of the tunnel. More and more of this unstructured text is becoming accessible and understood by machines. We can now search text based on *meaning*, identify the sentiment of text, extract entities, and much more.\n\nTransformers are behind much of this. These transformers are (unfortunately) not Michael Bay's Autobots and Decepticons and (fortunately) not buzzing electrical boxes. Our NLP transformers lie somewhere in the middle, they're not sentient Autobots (yet), but they can understand language in a way that existed only in sci-fi until a short few years ago.\n\nMachines with a human-like comprehension of language are pretty helpful for organizing masses of unstructured text data. In machine learning, we refer to this task as *topic modeling*, the automatic clustering of data into particular topics.\n\nBERTopic takes advantage of the superior language capabilities of these (not yet sentient) transformer models and uses some other ML magic like UMAP and HDBSCAN (more on these later) to produce what is one of the most advanced techniques in language topic modeling today.\n\n\ud83c\udf32 Pinecone article:\nhttps://www.pinecone.io/learn/bertopic\n\n\ud83d\udd17 Code notebooks:\nhttps://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic\n\n\ud83e\udd16 70% Discount on the NLP With Transformers in Python course:\nhttps://bit.ly/3DFvvY5\n\n\ud83c\udf89 Subscribe for Article and Video Updates!\nhttps://jamescalam.medium.com/subscribe\nhttps://medium.com/@jamescalam/membership\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\n00:00 Intro\n01:40 In this video\n02:58 BERTopic Getting Started\n08:48 BERTopic Components\n15:21 Transformer Embedding\n18:33 Dimensionality Reduction\n25:07 UMAP\n31:48 Clustering\n37:22 c-TF-IDF\n40:49 Custom BERTopic\n44:04 Final Thoughts", "duration": "0:45:14", "channel": "James Briggs", "upload_date": "20220511", "uploader_id": "@jamesbriggs", "download_time": null, "transcription_time": null}, "chapter_dicts": [{"start_time": 0.0, "title": "Intro", "end_time": 100.0}, {"start_time": 100.0, "title": "In this video", "end_time": 178.0}, {"start_time": 178.0, "title": "BERTopic Getting Started", "end_time": 528.0}, {"start_time": 528.0, "title": "BERTopic Components", "end_time": 921.0}, {"start_time": 921.0, "title": "Transformer Embedding", "end_time": 1113.0}, {"start_time": 1113.0, "title": "Dimensionality Reduction", "end_time": 1507.0}, {"start_time": 1507.0, "title": "UMAP", "end_time": 1908.0}, {"start_time": 1908.0, "title": "Clustering", "end_time": 2242.0}, {"start_time": 2242.0, "title": "c-TF-IDF", "end_time": 2449.0}, {"start_time": 2449.0, "title": "Custom BERTopic", "end_time": 2644.0}, {"start_time": 2644.0, "title": "Final Thoughts", "end_time": 2714}], "mp3_filepath": "c:\\Users\\happy\\Documents\\Projects\\obsidian-transcriber-service\\local/BERTopic Explained.mp3"}