

# Tutorials - Pandy Knight: def test_crash_course_with_pytest():
 Am I on? There we go. I don't have to shout now. My first PyCon was Cleveland 2018. Actually, Cleveland is not too far from here. Changed my life. Like, that was the first major conference I spoke at. I didn't realize how big PyCon was. I was like, oh, boy, they accepted me to speak. And then I show up and I'm speaking to a room of like hundreds of people. I'm like, oh, shoot. But that was pretty awesome. Since then, I've traveled the world speaking at different conferences. Lots of times about testing, sometimes about programming, sometimes about various other topics. Yes. I remember PyCon 2018. This was in the before time, in the before COVID when money was still free. I remember Instagram, Facebook invited a whole bunch of speakers to like a dinner. I'm like, oh, cool. It's going to be like happy hour. I'll get a free drink and maybe they have appetizers. I don't know. I'll just check it, check it out and see what's happening. The day before they send me a lift code to get there and back again. I'm like, oh, oh, they're paying for my ride. That's very nice. And then I look up the address and it's this restaurant called Crop in Cleveland. And apparently this was like one of the 10 best restaurants in the United States at that time. I'm like, holy crap. I get there. It's an old bank. When I say old bank, I mean, it's got like those marble pillars, that high ceiling, the fresco art. I'm like, oh, wow. Hi, I'm here for the PyCon Facebook, Instagram dinner dinner. And they're like, oh yes, right this way, Mr. Knight. I'm like, they know my name. They marched me straight through the restaurant and I'm just looking at like, I don't belong here. I only wore a t-shirt, this isn't right. They marched me straight through the kitchen. And I'm like, why are we going through the kitchen? Down the sketchy set of stairs into the basement. I'm like, I'm gonna die. This is a setup. I've been scammed. And then we go through the door that was the old bank vault. You know, these like 10 foot reinforced steel doors, right? Ten foot thick, right? Well, apparently they had turned that into a whole banquet hall in the old vault of this bank. And I get in there and then I see the Facebook, Instagram, and I'm like, Andy, okay, so I'm not going to die. This is pretty cool. And they proceeded to serve a three-course meal, open bar, like the appetizer were like ribeye sliders and wagyu this, and I'm like, what is happening? And they're like, here, have a beer, have another beer. Would you like red wine with your filet mignon? Why, yes, yes, I would went on and on it oh my gosh it was like what just what just happened PyCon happened that's what so I can't guarantee that will be your first PyCon experience but that was my first PyCon experience yes stranger things have happened but not to me all right are we three past the the minute mark yet oh I still have a minute to stall. I can still tell fun stories. But anyway, it was at that Instagram dinner that I met many of the other Python conference organizers, because there's not only PyCon US, but there are PyCons around the world. Has anybody attended a Python conference, not PyCon US, somewhere? Yeah, where have you been? New Zealand. Oh, wow, that's a long way. Recommended. All right. So when we all go to New Zealand, we have to go to PyCon NZ. Anyone else been to other Python conferences? Oh, wow. Well, there's a ton of them. I mean, every country seems to have one, even within the United States. I just came back from PyTexas last month. PyTexas, amazing. Oh Oh my gosh, it's in Austin. There's also Pi Cascades in the Pacific Northwest, Pi Gotham in New York City, Pi Ohio in the great state of Ohio next door. I tried getting a Pi Carolina started once, that failed. Thanks, COVID. Anywho, yes, enough of my stalling. It's 1.33. Who's ready to learn some PyTest today? Yes, that's all why you're here. If you're not here to learn Python testing, I'm sorry you're in the wrong tutorial room. That's okay. You can stay anyway. Maybe you'll learn something. I don't know. We'll try to make this fun. So yes, we will be learning how to use PyTest. But moreover, we will be learning how to do proper testing, right? To me, PyTest is just a tool, it's a very fantastic tool. It's my favorite test framework, not just in Python, but in any language. But really what I want you all to walk away with today is a deeper appreciation of building the quality feedback loop on the software that you build and on writing good test cases. So to do that, we will be building a test project from the ground up. We have no slides, none, no slides. That's right, this is all hands on coding together. Why we learn by doing. We don't learn by some boring, whoa, whoa, whoa. We don't learn by listen to some boring person just get up here and go blah, blah, blah, blah, blah. We learn when we have our fingers on the keyboard, when we're talking with each other, when we're trying things out, when we see it fail and we fix it to make it right. And so we will be marching through learning how the PyTest framework works, all the mechanics of that. write. very interactive. The way that I teach this is that it's like a slow moving train. I have tracks that I have laid for where we're going to go. We all get on the train in our cars and we just kind of roll slowly on through. I'll be doing a lot of talking and a little bit of coding. And as I'm coding along, what I invite all y'all to do on your laptops is to code along with me and give things a try. So that way that you can do it yourself, you can try it, you can see it fail, you can talk to your neighbor, hey, why isn't this working? Get it fixed. Now you can walk away with an example project of how you do the testing. If you choose to be a spectator, this is going to be like a four-hour long YouTube video. You can choose to do that too. I don't recommend it. Probably be very boring, other than my lame dad jokes, even though I'm not a dad. Try to make it fun, though. But as I've said, this is meant to be rather informal. This is meant to be a one-time kind of occurrence thing where we are in this room together on this journey. I invite you, if you have questions, please raise your hand and ask. This isn't so much of a presentation as it is a conversation, where I just end up doing most of the talking. You can help me by doing some of the talking for me as well. So please do ask questions. Don't be shy. Hopefully we can make this fun. Now, pardon me. There are some expectations I have as we go through this course. First thing is that I would expect you all to have basic Python programming experience, right? If if you have not programmed in Python before and you're here now This is probably not going to the best be the best workshop for you But given that we are at PyCon us I would assume most people have at least some basic programming of Python. You don't need to be an expert I wouldn't even need to say you need to be intermediate level for course. Or it could be that maybe you have a lot of programming experience in another language and then you come over to Python and just kind of pick it up as you go. That works too. Also, I would expect that you've, I can see you've all brought your laptops, which is very good, that you would have Python already installed. I mean, recent version, probably anything 38, 39, 310 above is going to be fine. I usually have to say, please remember to bring your laptops. I have done this tutorial at testing conferences where people don't come with their laptops. And I'm like, what did you expect to get out of this? I don't know. They expected to get something. They did not read the instructions. I can't help them. So we good so far? This all makes sense. Great. Any questions so far? Good. We've been paying attention. Cool. Awesome. So a little bit about me as your instructor. Hi, my name is Pandy Knight. I am the automation panda, bona fide Pythonista, prince of playwright, and speaker at many Python conferences. If you know me, you know I love two things very much. I love French Bulldogs. This is not my French bulldog, but this is art I generated from Mid-Journey and I wanted to share it because it looks like Andy Warhol. And I just went to the Andy Warhol museum today, as you can see I'm wearing a shirt. And I also like cars. This is my Volkswagen Beetle. It is very broken. We're getting there. It's okay. Ask me about that later. Yes. If you want to keep up with me, my blog, automationpanda.com, be sure just to Google me. If you search Automation Panda, you will find me. As far as the content we're covering today, as I said, I don't have slides, we're gonna be doing live code. But I have most of the example code online if you would like to reference it. If you go to this GitHub repository, let me, how do I share the thing? No, I do not want that. Give me just a moment here. Share, I'm gonna share the QR code so you can get it more easily. Share, create QR code, bam. So the repository is automationpanda.com slash tau intro to pytest. You can get the QR code here. This is simply for reference. If you were trying to do some of the example code and you didn't see it working on your machine, you can look at it here and give it a try. You do not need to clone this repository. We will not be working within this repository. We'll be creating a new project. But I just want this to be here for reference for you that you can have it if you need it, take it home, refer to that later as well. Also, I do want to shout out, has anybody heard of Test Automation University? Yeah. Yeah. So Test Automation University is an online platform where there are free courses from some of the world's best instructors all about testing and automation You can learn anything playwright Cyprus visual testing mobile Robot framework. I mean the list goes on and on testing your machine learning models The reason I call this out is because there is a learning path entirely dedicated to Python testing. And so if you take, there are multiple courses in this learning path. Oops. Let's see. Oh, no, no. So everything from learning Python programming, intro to Pytest, Web element locator strategies, and so on and so forth. So if you want to learn more about Python testing, more than we can cover in this three and a half hour period we have together, I recommend TAU. In fact, much of what we are covering today is in the introduction to Pytest course. testing to PyTest. I'm just the biggest fanboy of PyTests that I have found on the planet. So, anytime I need something, I go to PyTest.org. And so, if you have anything that you need, I recommend here. Cool. So, who's ready to get started? Yes. Yes! Excitement. I love it. Very good. Let us begin. Okay. Whoops. Let me do the thing. All right. Everybody can see this very So most people familiar, some people first time. Would most people say that they've got light experience with testing? Maybe not so heavy? Okay. That's fine. Good starting point. Like I said, what we're going to do is we're going to create our own new Python project to write all of our test cases in. But I want a caveat to say you don't necessarily need to create a whole new project to write new tests. In many cases, we already have some main project that we're working on. Like you could be developing a web app with Django or a service API with fast API. Or maybe you're building some machine learning model. All the different kinds of Python apps or whatever you want to call these little bundleable software units we develop, right? You already have some project for that, right? And oftentimes it is a recommended practice to put your test cases in the same project as the Coda tests. So if you've got like a Django app that you're developing, you probably want to all of your unit integration and tests in that same repository. Why? That way everything can be version controlled together. You can just check out one thing in a build process, build it all and then run the tests one by one. That way it kind of encourages your developers. If you're in an organization where dev is separate from test, it encourages more collaboration between roles, which is very nice. But it is not always, it may be a bit, it's not always the best thing to put your tests in the same project as the coded tests. Sometimes you might have a very large application made up of multiple repositories, maybe like a microservice mesh, or there could be very, very bureaucratic red tape boundaries between developers and testers. So you might have to make a separate repository for tests because reasons. We are going to create a new test project from scratch today simply because we are starting from zero. So we need a project to begin with. But I just want to give that caveat as to how, what we're doing here. All right, so where am I? Find your favorite place on your machine where you want to put your tests. I typically do this from command line because I'm old school like that. Did I? All right, so I'm going to make a new directory make dear PyCon Testing you can call it whatever you want doesn't matter I'm going to change directory into this PyCon testing. Yeah. Whoo-hoo and just like that we created a test project. Yeah, right typically we like to put our tests in a Folder called Tests. So, yay, we've got that. Now I'm going to switch over to Visual Studio Code because that's what I do. Open. You could probably create it through VS Code. I don't know why I don't. Somebody asked me that on stage one time. I'm like, I don't know. I'm weird. Everybody's got broken things that they do. This is mine. All right. So here, whoa, yes, I trust myself. Thank you. Okay. So new project, test directory, yay, fun. We need to also install pytest as a dependency. Pytest is not part of the standard library. Unit test, the other testing framework is, but we're learning PyTest. I think PyTest is better than Unit Test for many reasons. PyTest has this wonderful extensible plug-in framework. PyTest, you write tests as functions rather than classes. PyTest can iterate more quickly because it's not stuck in the standard library. The only downside is you need to do a pip install for it. Now, we could just do like pip install pytest like this, and that's cool. I'm going to set up a virtual environment because that's what I do. Do we know what virtual environments are? Most people? Good. Okay, cool, cool. For those who virtual environments may be new to you, most likely you're going to be developing and running multiple Python projects on your one machine. And sometimes what happens is different projects have different dependency versions. Like I want to get this version of Pytest versus that version of Pytest for reasons, right? Virtual environments let you control the dependency packages locally rather than globally for your whole machine. And so what I do as a recommended practice is I will create a new virtual environment for every single new project I create, and then I'll store those dependencies in a requirements.txt file. And the way that I do this is I use Python standard venv module. So I'm here in my root level of my project. I'll say python-m venv to invoke the venv module, and I'll put it in a folder called venv. Wow, creative, right? Simple is better than complex. Oh, I don't have Python installed. That sucks. It's because I have Python 3. I hate that sometimes. Okay, there we go. So now I have my virtual environment and what I'm gonna do inside my virtual environment, source, vnv bin activate. If you're on Windows, this command will be a little bit different. I forget what it is, but generally the flow is the same. Oh, I should also time out. If at any point you get stuck in today's tutorial, please turn to the person next to you and say, hi, my name is such and such. Could you please help me get unstuck on this little thing? I'm not sure what's going on. Be friendly, be kind, please help each other because I can't help 50 people in this room. All right, I could try. It won't go well. All right. So now I've got my virtual environment set up. You can see that it's been activated because I have this VNV thing in front of my prompt. And now I'm going to say pip install pytest. And that's the only dependency we need right now. And hopefully conference Wi-Fi is relatively expedient. Otherwise, this could be miserable. It's getting the job done. OK. Cool, cool, cool. And so now if I did a I If we wanted to save requirements.txt, I could say pip freeze and I could pipe that into requirements.txt. All right, now we got this and we can see, hey, look at all that. So if you ever wanted to check this out and reinstall the dependency like in a CI system or somebody new setting up the project, you can just do pip install dash R, I think, requirements.txt. There are other package managers out there. There's poetry, there's pipenv, there's I don't even know what else. I do this because simple is better than complex. And for what we're doing today, this is going to be just fine. Cool, cool. All right. So we've got our project set up. We've got our Python pytest dependencies. Let's go create a test module. So in the tests directory that we created, new file, I'm going to call it test underscore math dot py. Naming is important kind of when you have a when you're sorry, when you are writing test cases with pytest. Pytest has a discovery mechanism, whereas it will raid your projects to look for all the things that identifies to be a test case. And one of the things it looks for is these modules beginning or ending with test. So if I were to run pytest, it would say, ah, well, because this module is named test underscore, this must be a test module that has test case functions. Therefore, I will search it for test case functions. You can, I believe you can also do like math underscore test dot py. I just, I've always done test underscore because, because. Then furthermore, in here, we will define our test case. And so our first test case, we're going to say, it's's gonna look like this. It's going to be a function because in pytest, every test case is a function. Wonderful, it's not a class, it's a function. Yes. Def test underscore one plus one. Yes, all right. So simple test case, or simple function definition for a test case. Again, we want to give it a prefix of test underscore so that way Pytest identifies this as a test function inside of our test module. You can have non-test functions like helper functions inside this module too. But yeah, here we go. We now have a Pytest test case. If you're wondering what this pass command is, that's simply Python's no op operation. You can put pass to say, hey, I have a definition that does nothing. I'm doing nothing, right? Because Python doesn't have the squiggle, so you need to have a line of, anyway. We're not gonna keep this as pass because that would be a useless test case. What if we just hard code all of our tests to pass? Wouldn't that be beautiful? It doesn't work like that in the real world. I'm sorry, friends. No, believe me. Sorry, what? Empty doc string also works, yes. But yeah, I've seen people try to hard code pass into all their test cases. Look, the whole suite passes. I'm like, oh my gosh. You sold management on this, didn't you? Wow. Amazing. Things are broken in this world, my friends. All right. So how are we going to test one plus one is two? We're going to assert one plus one equals two. Woo! And just like that, ladies and gentlemen and everybody, we have written our first PyTest test case. Good job. Give yourselves a hand. Congratulations. That's all it does. That's all it takes to write a PyTest test case. It's a function that's marked as a test, and it performs interactions and does verifications. What interactions is it doing? Well, here we're just testing math operations. We're testing the addition function in Python, and we're doing verifications in that we are making an assertion that the outcome of that interaction equals a certain value. Last I checked, one plus one is two. But yeah, if anyone ever asks you on an interview, what is testing? Testing is interaction plus verification. You do something, you make sure it works. That's testing. Take that one home with you as a life lesson today. So some interesting things that we might see here as well. Not only is this a test function, but we also are using this assert command. And because it has pretty colors highlighting it, that must mean that this is a Python keyword. Yes, this is in the Python language. Basic assert statement. What an assert statement does, you take in a Boolean condition, which in this case is this binary operator saying, hey, does this equal this? And it will yield true if this equals and false if it doesn't. When an assertion command runs, if the condition passed to it is true, it'll keep rolling with execution like nothing bad happened, right? Oh, we're good. But if this is false, it will go kaboom and it will raise an assertion error, which will then go up the stack and you have to catch it under normal circumstances. We could also provide an error message. If you want to give some additional context to your assertion as to why something failed, you can provide a helpful message like this on the side. For most cases with pytest, I don't bother doing this. Assert is not a pytest statement, it is a Python statement. So you could use this statement outside of pytest. Now, if anyone has used the unit test library, any hands? Yeah. Unit test will have its own assertion library, you know, assert that this, assert that that. In pytest, we don't have an assertion library. We just have the plain old assert statement. Now, you might be wondering, Andy, wait a minute, hold on. Doesn't that seem like backwards? Didn't you say pytest is better than unit test? Like, but unit test has richer assertion library and pytest just has the simple Python assert command. We'll get back to why that's okay in a little bit. But anywho, let's give this test case a try if you haven't already done so. So let's run this test. I'm back at my command line. I am at the root level directory. I'm not in the test directory. I'm at root level. We should always run our tests from root level. And to run our pytest test, I'm going to run python dash m pytest. So I'm running the pytest module. And if I run it here, it will check for all tests in the entire project, discover them, execute them, and report back. Boom. And just like that, we have our first run. So we look at the output here, very basic. So you see this little banner, testing has started. Pytest will puke out all of its dependencies. We're at pytest8.2, that's pretty cool. The root directory from which it's running. Collected one item means it's found one test case in all of its discovery. And then it will print out the test modules that it finds. And then it will print out all the test cases it runs. If you see a green dot, that means it passed. Yay. Question. Yes. So in the discovery mechanism from the root directory, it will look in all the folders for any module that begins or ends with test. And then in those modules, any functions that begin or end with test with that underscore there. Yes. Oh, the command is right here. Yep. Yes, yes. I'm sorry, it's a small screen and a long deep room. So it's, I can only show so much text at a time. But yes, yes. So this, without any other parameters, this will start in the current directory and just go down. And so we see, we have this green dot. That means the test case passed. We can see 100% of the tests in this module passed. Yay. And then we get the final batter. One passed in 0.00 seconds. Yay. So not only have we written our first PyTest test case, we have run it and it passed. Good job, everybody. Yes. Great. If we wanted to be a little more... No, I'll save that for later. That's fine. That's fine. All right. So that was our first test case. And that was the happy path. We always liked the happy path of testing because that's easy and everything works and we're just so elated. But as we also know, testing can fail. And it's important to note that a test case that yields a failing result is not necessarily a bad test, but rather so long as the failure is correct, that is still a successful test, right? A successful test is one that runs to completion and yields the appropriate result. So we can have tests failing successfully. Let's see what happens when a test fails successfully. So let's write another one. Let's try test one plus two. Right? Now I'm going to be a little more loquacious in how I write this this time. I'm actually going to use variables, and you'll see why. A equals 1, B equals, oopsie, I can type, 2, C equals 3, assert A plus B equals C. Yes. Now, this test case, as we know, 1 plus 2 does equal 3, last time I checked. We're not in three- body problem. But if I were to change this value, this would inject a failure. What do we think is gonna happen when I run the test now? The test fails, kaboom! That's right, that was the easy answer, good job. All right, so let's run this again. Also, when we go to run this, I know some people might just be running it with a pytest command line. That should be okay for now, but the reason I always write it out as the full python-m pytest is because if you have other files in your project that you're trying to suck in and build with, the regular pytest command line may not work because you'd have to explicitly set Python path. If you run it Python dash m pytest, it'll suck in everything in your project. So you don't worry like, why is it not finding this module? That's why I always do it like that. So we ran the test and thingies went kaboom. Let's see what happened. So we see now I don't have a green, I have one green dot, that was the first test we ran, but now I have this big red F, F is for failure. When we hit a failure by default, PyTest will print a banner with all of the failures that it captures. So here's the failing test, test one plus two. The red color text also makes it pop out a little bit more. And so then we see, oh, wait, here's our test case. Here's the code I wrote. Test one plus two. I see the lines leading up to the failure and then boom. This little arrow indicates this was the line where the failure happened, which is no surprise no surprise right because we know that the assertion failed because the condition wasn't met But then beneath that is an even more interesting thing look at this What's going on here, what's the difference between these two lines? What do we see We see the substituted variable values. There is interpolation going on here. What we see at play is what is called assertion introspection. When there is a failure, PyTest will introspect into the reason of the failure, and it will take those variable values and reveal what they actually were at the time of failure. That's very, very helpful because if we didn't have that, if we just saw that this line failed, like, okay, that helps a little bit. But ultimately, what we might have to do is go and debug step by step through our test to see what the variables are at the point where the failure happens. Pytest just does that for you automatically. Beautiful. I don't need to rerun the test. I don't need to debug it. I can see exactly what was happening at this moment in time. Now, you might still have to go back and debug to figure out why those values were the way they were. But at least to have it shown to you, boom, will save a lot of your time with trying to figure out what's going wrong. Now, I understand that given that this is a very trivial function, you're like, well, the variables were printed right here. What's the mechanism behind the parentheses getting added? case this will still be printed out. Question? I don't know. Most likely I would guess as part of the introspection it's realizing, oh, just to make it very clear that this was being evaluated before the equals, I'm going to put that there. But that would be a question for the PyTest maintainers. Good question, though. So, yes. so this is why in PyTest we use the basic Python assert command versus any fancy assertion library, right? Because it's this command that allows PyTest to go in and do that assertion introspection. If you go to pytest.org docs, they have a whole page on assertion introspection. They explain what it is, how it works, getting in deep, all that good stuff. So light reading material for your homework. Cool. All right. So then we also see down here the nature of the failure, and that is an assertion error. Because what happens when this assert statement gets a false condition, it raises an assertion error, which is an exception type, and that goes up the stack. And so this is significant because you might be wondering, well, what is the mechanism that tells pytest there's a failure? Is there some sort of secret log behind the scenes? Does it just magically know something? No. In every single one of your test case functions, ultimately what determines pass or fail is whether or not the test case function raises an exception. If a test case function raises an exception that is not handled within the test case, pytest will capture it and report that means PyTest deems the test as passing. So if you want to fail a test case, raise an exception. How do we typically raise exceptions? Well, we're trying to do a verification, and so we use the assertions, and the assertions, if they fail, will raise an assertion error. Ta-da. You can have other failure types other than assertion error by raising other kinds of exceptions. But this is what we're doing here now. So anyway, we see the full introspection. We see the nature of the failure, we see the line it happened on, and then we see at the end of it, the short test summary where it's going to list all the failed test cases with the module, the function name in the module, and then the introspected line that failed. And we see one failed, one passed, that sort of thing. So that is how we have failing tests in Pyada. We're fixed yay now we have two passing tests and all that other mumbo-jumbo is gone because we don't need to see that because there was no failure. Very good, very good. Cool. So, with that in mind, the fact that failures in pytest are essentially unhandled exceptions raises an interesting question. What happens if we want to verify that an exception was raised appropriately? Right? Because there are times where we want to test happy paths as well as unhappy paths, and many unhappy paths lead to errors and exceptions, and we want to make sure that those are raised appropriately. And within a context of testing math operations, we could test divide by zero. Oh, that should raise an exception, shouldn't it? Yes, it should. If I try to divide by zero, what happens? Divide by zero error. Right, we can't take zero out of something. That just goes to infinity and beyond. Oh, math class. So if I were to run this, yes, boom, right away you can see, hey, this is what happens. We got that failure. This is a zero division error. We cannot divide by zero. That's bad math. But that's an expected thing. We want to make sure that we do get a divide by zero error. So the rudimentary way to do this will be like, okay, well, geez, I could do like a try, try this, and then try accept. And then, shoot, how do I accept? How do I do this again? Chat GPT, help me out here. One moment I'm slow It's just it's just zero to that's right zero. I've been doing a lot of Java and go recently There we go, so the lower what What's the little right corner Yeah, oh yeah, this. No, I was trying to remember the syntax for catching exceptions in Python. I've been doing a lot of Java and Go recently. How do I catch this again? Oh my gosh. I am not perfect. I may be your instructor today. I am not perfect. I don't remember all the things. I dance in too many languages at once. Makes me very sad. I wish I could be Python all the time, but that's not the world in which I live. It's probably not the world in which you live. Maybe it is. Maybe you live a better life than me. I hope you do. My life can be sad sometimes. But I'm OK because I'm at PyCon and it makes me very happy. Accept zero division error as E and then pass. And then here we want to say something like assert false. Right. We could do something like this, right? Like, hey, if I run this and I pass to the next line, that's wrong, I fail. But if I catch a zero division error, that means I pass. Or like if you catch another kind of, can I catch multiple exception types? Yeah. So yeah, I could do something like that. There's multiple ways I could do this. But this is very clunky, right? You can do it this way. I think in unit tests, you kind of have to do it this way. But by Jingo, PyTest has a better way. We don't want this ugly code. We want to do something more Pythonic. And so with PyTest, first I need to import the module because PyTest is going to give us some magicalness. So at the top of my file, I'm going to import PyTest. Yay, thanks Visual Studio Code for recognizing my virtual environment. And now what I'm going to do is I'm going to say with pytest.raises, and I'm going to put in the exception type. Zero. Oh my gosh, go away. Oh my gosh. No, no, no, go away. There we go. Wow. Zero division error. With pytest.raises, zero division error. Now what's going on in this thing here? Let me put the whole thing we're testing here. This with statement is calling this as a context manager. Are folks familiar with context managers in Python mostly? So for those who may not be familiar with context managers, this is probably an intermediate programming trick. Basically, a context manager is a fancy way that you can have special enter and exit logic, right? And so what is the enter exit logic going on here? Well,test is saying hey, you know, I want you to run this block of code and what I want you to just run it and if it raises the specific exception types that I'm expecting and declaring here then that means Everything is okay because I'm expecting that to be raised and we're going to move forward However However, if this code does not raise this expected exception type, then I'm going to have a problem and then I'm going to create an assertion error. And so it's essentially doing the same type of operation as we had before in our try catch, but in a more Pythonic way, in a more reusable way, right? This is much cleaner, this is much more readable, right? And much more standard, right? And so what we're gonna see here now is that, hey, when I run that, we should now see things passing. So let's save that, let's run the tests. Yes, and so now here we are, we're passing again, yay., we can be a little fancier on this too. We can actually capture that exception and check things on it. Capture that as E, and then after the context manager, we could say assert. Let's say we want to check what the error message was. What was it? Division by zero is in the string of the E dot value. So E is a context object. Value is the value of the exception. We're turning that into a string to get the string message of it. And then we're making sure this was a substring of that error message, which we should have seen here. Ta-da! Right? So you can check things about the exception too. It's not just checking that it happened, but checking that it has a particular message. Oh, I didn't save. Pro tip, save your code before you run it. Otherwise you'll be like, why is that error still there? Oh my gosh, I forgot to save. How many people have done that? It's okay. It's okay. From the most novice to the most expert, we've all done it and we continue to do it. Yes. So, ta-da. Any questions about this technique? Is good? Yes, question. Yes, yes. So there would be an assert happening in here. I'm assuming it's using an assert call. Maybe it's using some other kind of, maybe it raises a custom exception, I'm not sure. But essentially, de facto, there was a lowercase assertion assertion happening here and then there's this assertion happening here Yes The I guess yes, so Yeah, so if you if you have all of your asserts passing, test passes because nothing failed. Something to note about that though, is that the assert command, when it has a failure, it is a hard assertion. Meaning when you hit this, or like, let's say this failed, it's going to exit the test and it is not going to run anything that happens after. So keep that in mind. PyTests asserts are hard. I believe there is a PyTest plugin for soft assertions. If you want to like check something but not necessarily explode yet, that's another testing technique. I'm not going to go into too much detail about that today, but it's more about like, hey, how do you want to structure your tests? For most cases, hard assertions are just fine. Yes. Ah, so what we're doing with this particular test module is we are making sure that basic math operations in Python work. Now, of course, we wouldn't be doing this in a real world because we should be able to trust that the math operations work, but we're using this as a way to learn about the mechanics of the Pytest framework. So if I have a division operation, I want to test that if I try to divide by zero, that it successfully raises a zero division error. If I were to try one divided by zero and it doesn't give me an error, I would consider that to be a defect. I would consider that to be failing behavior. So that's why we are testing to make sure, hey, if I try to do a divide by zero zero it does actually raise the exception that I expect Good questions good questions Cool Any other questions at this time? We good. Shall we roll on down the tracks more? Let's roll on down them tracks. I like trains, but I like cars more. I like classic Volkswagens. So much fun. Okay. So let's do another kind of math test. Let's say that we want to test multiplication because that's the one operation we haven't hit yet, right? Def test multiplication. So I could, whoa, no, no, go away. So if I want to test multiplication, I could do something like assert, I don't know, two times three equals six. yes, thank you. But there are many other ways that we can multiply two numbers together. There are many different types of multiplication. I could also have assert, there's the identity property of multiplication where anything times one is itself. So if I do one times 99, what should that be? Oh my gosh. Who said one? Killing me. 99, right? Identity of anything times one is itself. There's also the zero property of multiplication Zero times 99 would be what? Oh I'm dead ah It's okay. It's gonna be zero Yes, who would that hurt that hurt in more ways than one? Right we could also multiply a positive number by a negative number What happens if we multiply two negative numbers? What happens? What do we get? These are the easy questions, France. It's okay to shout them out. All right, and then there's also, so far, these are all just integers, right? Well, we probably want to test like decimal numbers, or floats, or whatever we call them, right? So assert 2.5 times 6.7. Anybody know this off the top of their head? No cheating. 16.75, great. So great. So we have multiple different cases of multiplication that we can test here and that we should test if we're trying to be thorough in our testing. Each of these represents a different equivalence class to test. Ooh, big fancy word, right? When we look at planning our test cases, we want to pick different equivalence classes for variations of things. How many ways can I run a path or a system? What are all the possible bad inputs I could put in? How many ways can I multiply things, right? And the idea of an equivalence class is that you can't test every single possible variation, but you can group variations into categories almost or classes where they're kind of similar, right? And so with multiplication, we know the different properties of multiplication. And so these all represent different equivalence classes. We have the positive integer case, the identity property, the zero property, the positive times negative, the negative times negative, and the floats. Six different equivalence classes for just six different types of multiplication. We arguably don't need to test more than one value in each equivalence class That's what it means to be an equivalence class where it's the shared Attribute of being in that class is what matters not that you test every single value in there This is contrasted to a tool like hypothesis anybody's used hypothesis where they literally grind through every Single value you could possibly imagine they take the sledgehammer and say we're just gonna There's a time and a place for that. I have opinions about that. I don't think it's always as necessary So if you can identify equivalence classes much quicker much easier But there is a problem here because these are arguably six separate behaviors all under one test case and As we we know, given that these are hard assertions, what happens if my zero property fails? It stops. Are these going to get run? No. That is a loss of test coverage, my friends. Ooh, problem. We don't want to accidentally lose test coverage because of certain failures, right? Within testing, we have a principle rule that every test case should cover one behavior exactly, atomically and individually. Here we arguably have six behaviors under one test, which means we ought to have six different test cases. So that way, none of them is affected by the failures of the other. It's one of the beautiful things about the PyTest framework. If I have one test case fail, it raises an exception, whether it's in a search or something else, PyTest will capture that, clean up and safely move on to the next test so that we don't lose that coverage. Testing doesn't die just because you had one failure. You can keep on grinding through. But that gets lost if you start burying behaviors like this. So what we could do is we could do something like this. We could split this out into multiple test cases. We could say, test multiplication to ints, and then we could have another test case, test multiplication identity. We could have, whoa, I can't type, test multiplication zero, do something like this, and so on and so forth. In this case, it is because this is a trivial check, it was like, okay, it's not a big deal to do this. But there are gonna be times where you might have a more complicated function or class or method that you're trying to test or that the test steps are going to be rather numerous. And so to copy and paste the exact same operations over and over between different test cases may not be the wisest thing to do, right? Don't repeat yourself, D-R-Y. So what we find is in that case, it's like, well, if you split out these test cases, now you're doing a whole bunch of code duplication. That's not so good, but it's the same test procedure. Why don't we parameterize our test procedure? Wouldn't that be nice? And in fact, we can. So let's get rid of all this. Let's backtrack. We don't want this. We don't want this. Let us make one test function that can handle all the different variations. So how we're going to do, we're going to refactor this from our original test multiplication function. And what I'm going to do is I'm going to parameterize these values. There were three the that we expect, right? And so rather than writing out all these numbers like this, I'm going to parameterize them with variables and we're going to do the operation of the test with the variables. Oopsie. Assert A times B equals the products. Very nice, cool. Not products, product, cool. So we have parameterized our test case in the sense that we can now pass things into our test case But we need to set up what those values are that we're gonna pass in and that's all these values here So to do that what I'm gonna do outside of the test case is I'm going to Make a list I'm gonna call to call it products, plural. I'm going to make it a list, check it twice, going to find out who's naughty or nice because Frenchy Claus is coming to town. Yes, I can make bad jokes too. All right. So in this list of products, I want to get all these fancy numbers, right? And the way that we're going these We're gonna structure. This is a list of tuples Tuples are inside the parentheses instead of the angle brackets or the the square brackets. Excuse me and The tuples are going to match the order at which these variables are injected into the function So I'm gonna make a tuple a tuple. First one's going to be 2, 3, 6. Fancy. Right? Goodbye. Next one we're going to have is 1, 99, 99. Yes, magic happens. The next one was the zero case, so I can just do this. Zero, zero, bam. Right, the next one was three, negative four, negative 12. And what we're doing is this now becomes very data-driven, right? Like, we're putting in a list of all the values we want to grind through. This could be a short list, it could be a very long list. In our case, it's a list of six. Six tuples, six sets of inputs that are gonna get smashed into that test case like a boss. All right, 2.5, 6.7, and 16.75. Yes, yes, yes, yes. Okay, cool, cool, cool. So we have our list of data, we have our test function. The final thing we need to do is somehow tie this list to get put into this function. And to do that, we need our pytest module again. And we are going to add a parameterize decorator to this function. So decorators are pretty cool. You can use them to wrap functions in Python. Basically, it's almost like aspect oriented programming. It's like you can put a decorator on a function and it can do stuff before, after, or around. And so what this decorator is called, it's called pytest.mark.paramitize, and that is the British spelling, not the American spelling. Notice it is parametrize, not parameterize. No pytest maintainers are not going to make the American spelling. We fought that battle and we lost. It's OK. We're not bitter. And the pytest.mark.parametrize function decorator. Decorators are functions. The pytest.mark.parameterize function decorator, decorators are functions. The pytest-mark-parameterize decorator takes two arguments. The first one is a string that is a comma separated list of the variables by name for the function. So here we have a, b B and products. I'm just gonna copy that and put that in here. The second parameter is the list of tuples that we want to inject. And that was the list called products. So what PyTest will do when it runs this test case, when it runs this test case, it's decorated meaning wrapped by this decorator. This decorator says, aha. So I'm going to take the list of tuples. I'm going to line them up one, two, three with the names of the variables in the string. These names have to line up. Boom, boom, boom, boom, boom, boom. Technically, you could put them in any order as long as everything shakes out in the end, it's fine. Because there may be other things you inject like fixtures and all, and we'll get to fixtures later. But typically, recommended practice is keep them linear, keep them straight. That way you don't confuse yourself. And just like that, now we have a parameterized function that is data-driven, takes in all these, grinds it through, dependency injects, checks it, and it's happy. So, oh, what is this? So, when I run my tests now, how many tests do we think are going to happen? We should have six tests for that parameterized plus the other three tests. So we should have nine total. Ta-da! So even though this was just one test case definition, it counts as six unique test case instances because it is run once for each of the parameterized tuples and everything passes, yay. And in fact, I could prove that even with this, if I were to mess something up here, like if I were to say zero times nine is 99, it should still run all nine tests and yield only that one failure. So we can see that the subsequent tuples are not getting skipped because there was one of them that failed. on So, we're going to We've healed it all. Okie dokie. Any questions here at this point? We feel good about this? Yes, one question. Yes. Yes, because you could have helper functions inside your test module. question? I'm going to call it divide by zero underscore test. And it should identify it again. Oh, it doesn't. I thought suffix worked. Strange. I saved it. Okay. I was wrong on that. Never mind. I guess it has to be prefixed. Yeah. You can override the convention in pytest.ini. You can make any kind of convention for naming it. But I recommend going with the standard test underscore because that's default. And that's what most projects you're going to see are. Good questions. Question back there. Yes, hopefully. Oh, good question. There are other ways to view results. This is simply the default of what Pytest pukes out on the screen. There are plug-ins for generating HTML reports. There are command line options to generate a JUnit XML report that you can suck into your CI systems. We'll look at that a little bit later. But great question. Yes, you can do other kinds of reporting. If you thing I see people rage all sorts of opinions on this can is a test allowed to have more than one assertion? My answer is yes, right? Because what matters is that a test focuses on one individual independent behavior, not how many assertion calls it makes. In the process of exercising a behavior, there may be multiple outcomes that need to be checked. And so if that's the case, I want a test to verify a behavior, not necessarily verify every little thing separately about outcomes of the same behavior. So here we saw that with our divide by zero test. This arguably has two assertions, that it raised a zero division error and that the error has a particular message, right? I would see these as two outcomes of the same behavior of raising that exception but two assertions Right. You could argue that seeing a test case with Multiple assertions and I'm not just talking a handful. I'm talking boom boom boom boom boom boom boom boom You could argue that that might be a code smell. It doesn't necessarily mean the test case is wrong It means hey, maybe I should look into this a little more. Why do I feel like I need to have 27 Assertions for this one test because that that feels excessive that smells a little off Maybe it's masking something right? Maybe there are more than one behavior at play that I need to separate out. Or maybe you don't need to be checking all those different things. Maybe some things are silly and pointless. Excellent question. Cool. Any other questions? This is good. I like questions. Yes? Ah, yes. Oh, isn't that beautiful? Yeah, so you have one test that tests one behavior, and if that one fails, then you know all these other are gonna... Within the core PyTest framework, I don't think there's a way to set that up. There are some other tricks you can do. Like within PyTest, you can say like, hey, if you... And we'll look at this command later if we have time, but it's like, hey, stop running after the certain X number of failures. Like if I, let's say I have a suite of a thousand tests, you know, if I hit a hundred failures, just stop, stop the bleeding. We don't need to keep running this, right? Though I think what you are bringing up is more at the level of test orchestration, which would be the level above PyTest. That's asking the question, how do we structure and schedule our test suites well? Because what you might want to do is to say, hey, maybe I should run a smoke suite of like a dozen or two dozen tests. And then based on the outcome of that, then I run the full suite, but I filter based on the failures that happened in the smoke suite to know, hey, well, if this particular test fails, I don't need to run these 200. So maybe I can filter them out smartly to save some time. Because we just know it's just going to fall on its face. But yeah, that would not be so much a within-py test thing. That would be like a run your test, parse your results, and then use filtering commands to pick what you need. And we'll look at, if we have time, we'll look at the filtering and the tags and everything as well. Great, great questions. Okie dokie, shall we move on? Yes, we shall. Cool. So what we've done so far has just been an exploration of the mechanics of Pytest as a framework. Clearly, you do not need to test Python's math functions, right? They should work already. But there is one more that I want to cover. And this is one where I'm going to turn this over to y'all as an exercise. So get ready. We'll take about five, 10 minutes here. Test absolute value. What I would like for you to do, take five minutes, write a test case for the absolute value function. As we recall from high school math class, absolute value basically takes a numeric value and makes it positive. So absolute value of 10 is 10, absolute value of negative 10 is positive 10, absolute value of zero is zero, it just kind of has that V shape. So given everything we just learned about pytest, about parameterization, about exceptions, about all the passing, failing, assertion, blah, take a few moments and write a thorough test for absolute value. If I recall correctly, the Python function name for absolute value is abs. Is that right? Very nice. Ha ha, I can remember things. All righty. Well you do that I'm gonna get some potassium Okie dokie So how we feel about that? For doable pretty straightforward not too hard Yeah, so here's how I would have done it. I Would have done the same kind of thing we did with the multiplication Probably made something like what's it called? Numbers I don't know I'm lame naming things is hard, isn't it? And I would have done something like, okay, well, we have our positive integer case. We have our negative integer case. We have our, I don't know, float value positive case, and maybe like a float value negative case. Something probably like, oh gosh, wow, where did that zero come from? Something like this, all right, and then I would have shamelessly copy pasted this line. And I would have been like, okay, so I have my number and my answer and Numbers like that and then number Answer and then something like assert absolute value of my number Equals my answer Yay. Is this similar to what you all had? Yeah. So we've got our equivalence classes. We've got our parameterization. We didn't need to worry about any sort of exceptions out of this one, thankfully. I'm pretty sure absolute value function doesn't throw exceptions. Though I suppose we could have done another test of like of like hey what happens that we put in a string? Right, that should probably yield an exception, but I'm not gonna do that for the sake of time if we run the test Oh something failed. Oh because I I put in the wrong That's good. My tests caught it. Yeah. Okay. Now we're good Okay, cool cool very very good. Very good. So Enough with these math functions. That's not very interesting. Let's test something a little more applicable So now we're going to shift our learning from how does pi test work to how do we actually do testing with pi test? And if we're going to test something that means we need to write some code to test first, right? Yeah, do we have question? Uh-huh. Ooh. Ooh. So let's say you put in a 10 and you got a 9. Ooh, yeah. There's an art to that, right? For math functions, do we want math to be precise? Yes, then we should probably be precise. Otherwise, we're not going to get our spaceship going to the moon again. Not good, right? If you're on a web page and you're off by five pixels, is that important? Probably not. Maybe you want to be a little fuzzier with those assertions. So, know your domain, make wise choices. Remember, the whole point of the test is feedback. Right. It's trying to tell you and your team as the builders and creators of the software that you're making, is this good or not? Right. This is meant to be an alerting mechanism. And so if you're going to be writing tests, if you're going to be taking the time and investment to do this, this takes extra time. Copilot can't get this stuff right. We're not there yet. If you're going to be making the investment to do this, make sure that you're getting the feedback that you need. And so is it adequate just to be greater than zero? I don't think so. Not in this case, because if something wonky happened and gave me the wrong number, that's not good. That's a failure. I would want to know if that happens. Yes. Still in question though. Testing is an art as much as it is engineering. Never forget that. That's why it's so opinionated. Okay. Cool. So as we were saying, if we're going to be testing if we're going to shift into learning how to do testing on code, then we need to write some code that's worthy of testing. So what we're going to do is we're going to make a Python module that we're going to test together. It's going to be simple. Nothing too crazy here. We said we're going to keep it more like beginner level Python programming. I mean double underscore. Anytime you hear somebody saying dunder in the Python world, it means you're two underscore characters before and after whatever that name is. Oftentimes this means something special is happening, whether it's a special module, function, who knows. Who knows what this dunder init py module does? Why am I creating this? Anybody? Turning this into a package. That's right. We're going to leave this blank. Strange if you haven't seen it before. I know. I was weirded out the first time. I did this. I'm like, wait, what? Because Python is special. But all that means is now this folder is a package that we can use for imports with other Python modules. You can also do other things in Dunder and NITpy. Very dangerous. I don't recommend it unless you absolutely have to please avoid anyway i am now going to create a new folder inside of this stuff package it's no longer a folder it's a package and i'm going to call it a cume dot py accum and in this wonderful thing i'm going to make a class called accumumulator. Yes, we're doing object-oriented programming now. It's okay, don't be too scared. Okay, I'm gonna have a dunder in it, which is kind of like a constructor, but not. It is a initialization method. There's a nuance in Python. Sorry, Java friends. I feel the pain. I wish I could come to Python more. I had a whole project, my team written in Python, they're like, write it in Go. And I'm like, why? Very sad. Very sad. I love Python too much. Maybe it's a problem. OK. So what we're doing in this one is I'm creating a private count variable. I am sticking it onto self, meaning this is now a variable of this object that I'm initializing. And I'm giving it an initial value of zero because when we start counting, we start with nothing. Just like with this test project, we started with nothing, building it up. I'm also gonna add a property for counts. Wow, property. I can type. Define count. No underscores here. Passing in myself, so I'm attached to this class. count. This is a single underscore. It means this variable conventionally is private. Nothing is truly private in Python. Like, you can get it if you really want to. But this kind of tells other programmers, like, hey, you should keep it private, right? Certain things you should keep private. Like, my disdain for other. I'm not going there. Whoa. Almost said bad things about other languages that I don't like as much about Python. That's okay. All languages are good, but we just have our favorites sometimes. It's okay. It's okay. So, we're going to have another method, add self, and we're going to pass in more. And what we're going to do is say self underscore counts plus equals more. So, I'm going to add more to the value of counts and accumulation. Furthermore, I'm also going to set a default value for more to be one. So that way you can say, hey, if you just say add without any arguments, it'll add one, or you can override more to add more than one at a time. All right, so fairly straightforward Python class, nothing too fancy going on here. We'll leave it here for now. Does this make sense? Does this seem okay with everybody? Yeah, nothing outrageous. All right, so what we are going to do next is we're going to write unit tests for the accumulator class. Now, when I say unit test, what does that mean? A unit test is a white box test that covers methods and functions in Python. That is my definition of a unit test. Some people define to be like, well, unit test is just any little unit of behavior. And I'm like, that's cool. But people abuse that definition to be things, well, a webpage can be a unit. And I'm like, no. What makes unit tests important is that first of all, they are white box, meaning the tests have direct access to the code that they're testing. Meaning our test cases are calling the methods and functions versus like a web UI test where you're calling a webpage, that's black box. And also with a unit test, it's the idea that there are no external dependencies, that everything covered in a unit test is entirely within the code, right? We're not calling out to a database, we're not calling out to an API, we're not calling out to the file system, we're not calling a webpage. It's all within the code. And so that way our unit tests can be entirely contained within the project themselves. They can be built at the same time the product code is built and they will run extremely fast. Order of magnitude, a unit test should take about a millisecond to run, right? You can run thousands of milliseconds or milliseconds, wow. You can run thousands of unit tests within mere seconds versus if anybody's done like API testing or web UI testing or mobile testing, that can stretch into minutes, if not hours. We'd like unit tests to be part of our build pipeline. Anytime you build an application, you should run your unit tests and you should yield code coverage reports as well as test reports because it shouldn't be that much more time. Unit tests are there to keep you safe. They are as close to the code as you can possibly be. So, we always, always, always write our unit tests, right? Anytime we write code, we write unit tests to cover the code, right? Thumbs up? Yes, yes, yes, we do that. We do that. We should do this. If you don't do this, you will start doing this now today. If you don't write unit tests for your code, shame on you. Start now. I will shame you into writing unit tests. I have had so many people writing not writing unit tests, and it breaks my heart. We are not going to be those people. We're going to be the good people who write unit tests, right? Yes? You're not being very convincing right now. Are we going to write unit tests for our code? Okay, that's the best I'm going to get. I know it's after lunch, I know we're tired. You've had two days of tutorials now. It's okay. All right, so I'm going to say, whoa, oops, oops, oops, wrong screen here. I'm gonna create a new test module in my tests folder called, oh my goodness, test underscore acum.py. Now I tend to like, for unit tests, I like to have my module name that's under test to match the test module name, just to line things up. Typically, I also like to kind of mirror the package hierarchy if possible. Since this is a tutorial demo kind of thing, like I'm not going to make it more complicated with all that. But if this were in like a bigger project, I probably have tests and then I have a folder called stuff and then I'd have test to cumin there. Other thing I should call out, while we did make the stuff folder a Python package, it is typically recommended with pytest that you do not make your tests folder a package. Notice how there is no init py under here. It can cause some wonky behavior and how things are looked up and all that. The discovery mechanism will work even if it's not a package. The only time you might want to make some stuff under your test folder a package is if you're trying to import between different test modules. I don't recommend that. If you've got stuff to share, you should either put that in a comptest.py file or you should be making another package to share. So test should be tests. Okay, so test underscore accumulator py. What I'm going to do is I need to import from stuff dot accum, import my class accumulator. All right, so we're going to write some tests here. There are a couple different methods in this class that we should be testing. So let's test them all. First thing I want to do is test underscore, oopsie, oopsie, oopsie, test underscore accumulator init method, right? Pass. Other things I want to test, I want to test the test. Sorry, wow, def test accumulator. In fact, I'm going to copy that just so I can keep that. Test accumulator, the add method, and I'm going to do the variation of adding one. Pass def test. Oopsie, wait a minute. I should have just pasted there we go test accumulator. I want to test the case of adding Three right because we have that more override pass and I want to test hey what happens if I call oopsie? Oh my gosh. I'm not good at my own copy paste What happens if we call the add method twice we say add and then we say add? test. And then there's also one more fancy case of def. Let's see. We had that, if you recall, we had this count property. I should not be able to set the count property directly. It should yield an exception if I try to do that. So let's make sure that we can actually do that behavior. So test accumulator cannot set count directly. Okay, so what I've done here, before writing any of the actual test code, I've just kind of enumerated the different test cases I want for this. And this is a strategy that has helped me time and time again to just kind of say like, hey, I want to identify all the behaviors and get that mapped out before I go in and get lost in test code. And so here we now have enumerated one, two, three, four, five different test cases. Let me stretch this down a little bit. And yeah, so now that we've, does this feel good? I feel pretty good about this. Cool, so now let's go and implement some of these. So given that we are testing a class with methods instead of a function, we need to construct the object for this class. So something like a cume equals accumulator, like that. There were no initialization parameters. Great, so we've constructed the objects. And now, you know, that's the init that's happened. I wanna say something like, I wanna make sure that it starts at what value? Zero, right? So I'm gonna say after initialization assert a cume.count equal equals zero. Ta-da, fancy. Okay, so now we have these other tests. Add one, add three, add twice, right? I'm gonna let y'all go for a couple minutes. Try to implement these test cases on your own. It's the same kind of thing. You're going to create an object and then call those methods and make sure you get proper outputs. So take about three minutes or so, give that a try, see how it goes. And I'm going to finish up my banana. Is everyone doing okay? All right. I'll give about one more minute and then we'll review together. Is that cool? Yes. Okay. Very good. When the clock strikes three o'clock. Oh, yes. As a reminder, this code is available in the example code repository I showed before, automationpanda slash tau dash intro dash two dash pytest. And this one is empty. Okie dokie. So whether or not you're ready, I'm ready, so I'll show how I have implemented these So the annoying thing is in every single one of these test cases we need to have a reference to the accumulator object You might think well. Hey, you know why can't I just create this object one time out here and have it shared by all of them? That is very, very poor testing practice. Every test case should have its own object created because if you were to share objects like this, that could break test case independence. You could have things from one test case lead into another test case, and we do not want that. So no global objects, just like no global variables. Yes, it's a little repetitive to have every test case create its own object, but that's okay. That is what we want. So if we start to come in here with all these others, created the accumulator class, we're trying to add one. So I could say, Accum a cube my object dot add By default it has the value of one being added and so now assert a cume count equals One because I started zero I add one I expect to get one for my add three However, it'll be like this so add this time I want to provide a value to override the default more. And this time I should be getting the value of three at the end of it. If I'm adding twice, what that means is I'm saying accumulator.add and then again accumulator.add because the thought is if I'm at zero, I add, I get one. If I add one more time, I should get two. So all those fairly, fairly straightforward. And then finally, the last one, this one's going to be a bit more painful. For this one, you do need to import the pytest module because we're going to be catching an exception. Import pytest. And in here I'm going to say with pytest.raises. And if you know how properties work, you'd know this is an attribute error that it should bleed out. And if I were to say a accum. count like this, trying to directly set this property, that should explode. Now I can do other things. I could have done like a match on the raises. I could have got the message afterwards and verified things about it, but I think this is okay enough for our purposes here. And then of course, after we write our tests, every time we write a test, what do we do? We run it to make sure it's okay. And boom, just like that, now we see we have two test modules, testocume.py and testmath. We added five new tests. We still have all of our old ones there. And total tests passing 18. Very good. All righty. So let me drop the terminal again. Come on down. Yes, there's a question in the back. Oh, in the terminal. Yes. Oh, shoot, yes. So this percentage represents how much of the percentage of the test suite has been run by that particular moment. So once you finish the test, the QM.py, these five tests account for 27% of the 18 total tests that have been discovered. And so by the time it hits here, after the test math is done, now it's run 100% of the tests that it discovered. That's what that's representing. Good question. Cool. Other question? For the... This one? So there's no explicit assert command because this with pytest raises block is implicitly performing an assertion. The assertion is, hey, if it, it's asserting that this particular call raises an exception of this type. It just doesn't have the assert command in there. Question in the back here. Not directly, but I can discuss coverage. Do you have questions about coverage? TLDR, pip install pytest-cov. Ta-da! Use command line options to get it. Should you add test coverage? Yes. If you're running unit tests, it doesn't matter what language, what framework, you should always just run code coverage with it. Done. Is 100% code coverage what you should strive for? Probably not. You should be trying to get as high of code coverage as possible for where it makes sense. Don't cover boilerplate stuff. Cover unique domain logic that you are writing. You should be covering 100% of the unique stuff that you're writing most likely? I'm sorry. Add three does. Yes. So say that again. We're writing each test case as an individual function. No. Don't do that. No, this is pytest. that's unit test stuff. Pytest, tests, or functions. In theory, you could shove them in a test class and you can actually run unit test test classes with pytest, but I think putting these in a class is bad practice because you have a much higher likelihood of like writing confusing code. You have a higher likelihood of possibly violating test case independence. No functions for the win Functions for the win a test is a procedure a function is a procedure, right? I never understood why in all the object-oriented fervor of the 1990s people thought it was a good idea to shoehorn the idea and construct of a test case into a class structure. Made no sense to me. X unit style frameworks like X unit, J unit, all that stuff, I don't like how that's designed. Pytest is the only one that stands out. Why? Because functions model test cases more naturally. Readability counts. QED. Question here. So in any given test, you will have to do some setup steps, right? To put whatever you're testing into the initial expected state from which to exercise behavior. In our case, it's as simple as writing this or as constructing this accumulator objects. Sometimes it could be that like you have to navigate your web page to a certain or your application web app to a certain page. It could be in some cases you have to populate a database. What I recommend is doing the minimal amount of setup you need for any given test case. So like the more that you have to set up, the more complicated things become. I've seen a bad habit where people will, let's say like, they'll spend like 10 minutes setting up all this giant stuff for a set of test cases where some of those test cases might not even need all the stuff that's set up. Don't set up more than you need, but definitely do set up what you need. Oh, guess what we're going to talk about after the break. Someone's jumping ahead. Amazing. Good questions, though. Any others? Yes. Right now. You scroll down to the exception panel. Yeah, there you go I'll be around. I need some water. I need to sit down But yes, please be back here promptly at 3 30. I will not be waiting. I will not be giving extra time And just like that it is time it is 3 30 Welcome back everyone. Do we feel refreshed? Did we get some of the chocolate brownies? They were pretty good. Yeah. Somebody's... Take your seat quick. We're getting started. Yes. Cool. All righty. So, before we kick in to Pytesting again, I just want to share some cool things about Python and the Python community. I'd asked, how many people is this your first PyCon? I was actually pretty surprised to see most of you. This is your first Python conference. So again, I say welcome to PyCon. You have no idea what's coming in the next three days. It's going to be fan-freaking-tastic. You're going to go to that expo hall and walk away with more t-shirts and socks and giveaways than you can stuff in your carry-on bag to take home. It's great. You're gonna have so many talks, you're gonna be sick of them by the end. Make sure you check out Open Spaces too, they're a lot of fun. My first PyCon was PyCon 2018. Hard to believe that was six years ago. That was in the before times. It was in Cleveland, Ohio. I actually wrote a blog article about my experiences. If you're interested, capture the QR code, give it a read, give it a like. It is no secret that this event made a huge impact on me, for me personally and professionally. I don't think I would be the automation pandemic you know today had I not gone to PyCon 2018. I gained so much inspiration that still fuels me to this day. I made friends that I still have today and I still look forward to meeting every year at PyCon US. So for y'all who it is your first time, definitely make the most out of this opportunity you have this weekend. I hope that you can learn a lot. I hope that you can be inspired and I hope that you can make some lifelong friendships too. Also, whoops, come back, no. During today's tutorial, I've been making quips like beautiful is better than ugly, simple is better than complex, readability counts. These are not things that I have originated. They come from something called the Zen of Python. If you haven't heard of the Zen of Python before, this is a set of guiding principles for the Python language. If you want to see what the Zen of Python is, you could Google it, or you could open up a Python interpreter and say, import this, and you will find the Zen of Python. It's a fun little Easter egg. Oh, come on. Import this. Bam. And so you get, I think it's 20 principles. One of them is unspoken. But these are things that have not only guided Python, but when I look at these, they guide how I make software in other domains and other languages as well. I spent a lot of time doing.NET C sharp development. Right now, I am working on a Java project. I've just rolled off of a Golang project. nevertheless I carry these principles with me, and they help me write better software So go to open your Python interpreter import this you will find it Very nice very nice alright, so let's get back into the pytest coding whoo So before the break we wrote some unit tests on our accumulator class. Again, nothing super duper fancy. We had, what, five tests? And we set up the class, we made sure it got set up right, we added a whole bunch of stuff, we made sure we handled the property properly. But when we look at these tests, there's one thing that kind of grinds my gears a little bit. And that's this duplication I see. Every single one of these tests is constructing an accumulator object. And that's repetitive. It's necessary, but nevertheless repetitive. What I would really like to do is kind of pull that out into some sort of like setup Routine right because with any kind of testing we're saying before There's always going to be something you have to initialize there's something you have to set up whether that's creating objects or loading data or opening a web page or Whatever there there is there are setup operations and many times there are also cleanup operations too. Unit testing, you typically don't have cleanup operations because it's like, well, the objects kind of get reclaimed, right? But if you're doing black box style testing, which we'll be doing later, like if you're, let's say you're using Selenium WebDriver to open up a browser, maybe you still use WebDriver instead of Playwright, I don't know. But if you are, your setup is open the browser and your tear down is close the browser or else it's going to be a rogue zombie process. It would be really nice if we had a way to handle those setup and cleanup operations. If we were doing an X unit style framework like unit test, then we'd have our test class and we'd have methods for the test cases, and then we'd have a setup and cleanup method in there. But given that pytest is all functions, how can we possibly have shared setup and cleanup? And the answer to that is with fixtures. Has anyone heard of pytest fixtures before? A couple? Yeah. How many of us know how they actually work? Yeah, some of us do. Great. I didn't know it first. Then I learned them, and then I thought they were great. Fixtures are one for Everything's a function in pytest, whoo, right? What a fixture is, it's a function that will do some setup and possibly some cleanup. So to make a function, I define the function. I'm gonna call it a cume. And if this is a setup fixture, we're going to do some stuff and then return an object. And that object that we return is the thing that we set up. So in this case, what do I need to set up? I just need to create an accumulator object. So I'm going to say return some new accumulator object. Whoo, fancy. You can do more rich things than this. That's OK. Like if you wanted to have multiple steps, it's fine. But for our case, for this fairly basic unit test, this is all we need. The other thing I need to do to make this a fixture and not just some random helper function is I need to attach a decorator to declare it as a fixture. So pytest. fixture, bam. And just like that, we now have a fixture. It's a function that does pytest magic for us. To use my fixture, all I need to do is I go to my test function. And in the argument list, I declare that fixture by name like this. Boom. That's it. I can get rid of me explicitly setting up that object. What magic is happening here? When PyTest discovers the test case, this test case here, it will look at the list of arguments. And if it sees arguments there, it's like, hmm, I have to figure out where these arguments come from. Sometimes arguments come from parameters. We saw that before with the PyTestMark parameterized, but sometimes they come from fixtures. And so if PyTest finds like, oh, wait, I've got one that's not a parameter, PyTest will look for a fixture of this same name in the available scope of fixtures. And here, oh, here's a fixture with that name. What PyTest will do is it will call this function to run it, and it will take the return value and it will inject this return value as the object by this name, as a variable, into this test case. That's how setup works in pytest. Ultimately, what's happening is when I declare this, I'm saying, hey, I want you to run this as my setup routine and make the object by this name. So I can run the rest of my test steps assert a cube count equals zero because it's that object that I was constructing and Now I can refactor all of my tests like this I can say accumulator accumulator and every single one of these because every single one needs one Right and then I can get rid of the repetitive Constructing call and all of that. So now my test cases are tighter. They focus less on setup steps and more on behavior exercise, because I've separated that concern. This is very reusable because any test that needs to set up an accumulator can call this one. And yeah, it's basically just magic dependency injection. So if I save and run, functionally I haven't changed this test, but the code is just better now. And that is all there is to fixtures. Pretty sweet, right? Question. Ah, good question. So, where can this fixture be used? The way I have written this, yes. Because this fixture is in this test module, then only test cases within this module can use this fixture. If I were to create a separate module, like this test math, you're not going to be able to use this fixture in those. But that doesn't mean you can't share fixtures. If we wanted to share this fixture outside of this module, we could. What I could do is I could come up to my test directory, I could make a new file, and I can call it conftest.py. This is a special module. I didn't come up with this name Pytest did. Can you change this name? No. This is Pytest. You got to roll with it. Conftest.py. Conftest.py becomes the dumping ground for all the fixtures you want to share. If you've heard of PyTest plugins, you know what PyTest plugins are? They're just fixtures. Oh, really? Yeah, that's right. Oh, now you know what a plugin is. We'll get more to that later. But essentially, I could move this. Of course, I'll need to import PyTest over here. Oh, and I'll need to import.test over here oh And I'll need to import actually I can cut and paste that out So now oopsie. I don't want debug console now This fixture can be used by any module underneath of the containing directory of the conf test.py module so I It can be used by testacuum, it can be used by testmath, and it could be used by any modules and subfolders as well. And I'll prove it by running it. Magic, still runs, right? Now, was it proper practice for us to put this fixture in a shareable place? That's debatable. Most likely, this fixture is only ever going to be used by this test accumulator module. So we don't necessarily need to move it over here. But you could if you wanted. I don't think it's appropriate, so I'm actually going to move it back. But note that if you wanted to share fixtures, that's how you could do that. Moving it back should still be okay. Right? Good question, though. Another question. What do you mean it's built into the language Oh You like in the same like in the same directory layout it's like you have your go module and then go.Test, that kind of thing. or just about what why fixtures are cool in just a moment here, but like that this is magic because it's like I can use that fixture anywhere I can use fixtures to build fixtures The one of the biggest limitations when you have a test case class with a setup and teardown method is that? That setup and teardown logic is stuck inside that test click inside that test class you want to do in another test class You got a copy paste. You gotta have some weird mechanism for doing that. Fixtures are scalable, fixtures are shareable. You can use that anywhere. Just put it in the shareable location. This is the magic of PyTest. This plus plugins. Just make PyTests so incredibly awesome. Another question. Yes, yes. So, if I can dig into this a little bit here. The way that this is written, whenever a test case calls a fixture, this fixture has default scope. We haven't provided scope. So what that means is that it's called fresh per test case. So every test case is calling this accum method or this accum function again, and therefore getting a new accumulator object. So with that, these are each independent, right? There's no overlap of sharing with this accumulator object. Now, if for some reason there was some object that you wanted to share across test cases, where there would be one instance of it created and then injected into every test, you can do that as well. And the way that you do that is you set the scope. Scope equals session will set up an object one time. The first time a test case calls that fixture, it will run it and then pytest will cache the value and any other test cases following will simply receive that value by dependency injection rather than running it again to get a new value. Why might you wanna do this? Well, you probably never wanna do this with unit testing, but with black box style testing, there may be certain things that are shared. For example, if you're doing an API testing project or a web UI testing project where you're testing some application that's deployed out there, most likely you will need to read in environmental information, things like usernames and passwords, things like URL addresses for things, right? Possibly even secrets from a secret manager service like Doppler or Azure Key Vault or something. You don't want every single test to have to read them in every single time. What you want to do is you want to read them in once, treat them as immutable, and inject them, right? Because you don't need to read a config file 500 times for 500 tests. You need to read it one time and then just pew, pew, pew, pew. That would be a case where you probably want to use session scope. Read it once, inject it. The key thing is treat that object as immutable. Because if you start to mutate that object, you start to make changes to that object, that's when you can start to have side effects run rampant throughout your tests. So in this case, we do not want session scope. We want function scope, which is per function, meaning per test case. And honestly, that's the default. So we can take that off. If you want to learn more about scope, here we go. Fixture, def scope. Shoot, that's not helping me. I should just, fixtures, here we go, this is what I'm looking for. Scope. So you can learn about all the different kinds of scope in the PyTest talks. There's more than just session and function, but honestly, those are the two that you probably need to use the most. Question here. Does session scope trigger an error if a function tries to modify it? Is not gonna stop you because it's like what? This is why there's a difference between Knowing how a test tool or framework works and knowing how to do good testing I'm trying to hit both today Right because I have seen people do scary and abusive things with their tests I have seen people be like well I need to run this test first to set up the data for that test No, don't do that. You know, I've seen people like oh, well, I need to run this test first to set up the data for that test. No, don't do that. I've seen people like, oh, well, I need to, after every step, wait 60 seconds for the system to be ready. Oh my gosh, use smart waits, people. Wow. Or like, oh, I have a test that has like 127 steps. It goes this, then this, then this, then this. No, split them apart, please. Yes. Nothing will then you make copies of that golden state that you inject Right. that way you're not duplicating the parsing, you're just copying the data and then injecting a copy of the data so that if a rogue agent screws up whatever is in that object, that's okay, you still have the original. Nobody gets to touch the original. Correct. If you can't protect your data to be immutable, like in Java, you just declare it final, or C sharp, declare it constant, or something like that, or like public and private access modifiers. If you can't do that, because Python, nothing is private, right? The better strategy for safety is, OK, build your thing, make a copy, and then inject the copy. So there's a question over here. What are your thoughts about where to the database, whether that's from environment variables, whether that's from some config file, some INI file, source doesn't matter, but ultimately there would be a fixture of read my config. Then there would be another fixture I would have that would be, hey, you know, create a database connection that would call the first fixture and then use the values to then set up the database connection. It's like a session object or something. And then the return value for that fixture would be whatever session or connection object. You can have fixtures call fixtures. So if I were, let me sketch that out real quick. I mean, I could do something like, just to kind of illustrate what you're asking with code. Fixture, I could say something like def reconfig, and I mean, this would probably be, this would be a good example of when I'd want to use session scope. Scope equals, for the whole test suite, you know, the whole test session. I'm not gonna put in the code for that, but just imagine reading an environment file. And then you could say the database connection fixture, I don't know, DB. And then you could say, actually, I probably wouldn't want to say it as a verb. I'd want to say like config, like a noun. Fixtures should typically be nouns because it's like you're getting a thing. So maybe I could say dbconfig. dbconfig and this reads dbconfig from some file or vars. And then this one is connect to the DB connection. I don't know. You do whatever that, return connection. Return, I don't know, something. And then here, the magic would be dbconfig. So you could have this fixture calling that fixture. And so you can separate your concerns. Are there ways of passing the command line arguments into fixtures? Like, the path to the file that you want read from. In most cases, what I have found with testing projects is like you don't need to provide the tester with the capability of moving around their file. They just, they need to put it here, put it here, give it this name, you're good. You have to justify why are you giving the testers that much control. Is that something they really need? Maybe it is. Maybe because you're on some weird CI system, it needs to be in this place versus that place. I don't know. Maybe? Maybe that's a reason why. But what I have found usually is like, you know, I haven't needed to worry about command line args too much. And also I would say in other programming languages, other domains, most test frameworks do not allow custom command line arguments. And so you don't have that option. People are forced to do either environment variables or config files. And so it's like, you know, for me, I dance between all different stacks, right? I have my preferences. But most people who are doing testing who come into Python versus Python people doing testing that nuance makes sense most testing people who come to Python might not even realize they can do custom command-line arcs and so it might not even be in Their mind to do something like that. They would be more accustomed to be like, yeah, let me just environment variable, right? But that's real good questions, though. Real good questions. Cool. All right. I'm going to move this down if that's okay. Okay. Another, actually, there is another thing that I can share since we were talking about the database connection. I had mentioned before how fixtures, we can use them for setup, but we can also use them for cleanup operations. The way that we do that would be actually within the same fixture method or fixture function. For example, here this is establishing a database connection, right? As far as a setup of a test, we probably want to create that and open it and then it stays alive throughout the test. But after the test is concluded, we probably want to cleanly close this database connection, right? Pretty typical kind of thing. What we can do to do setup and cleanup within the same fixture is to turn this fixture function into a generator. Ooh, who knows what generators are? Some of us, yeah, okay. So in Python we have this cool thing called a generator it's it's like a a Function that creates a list as you go, you know If you if you just create a list one, two, three, four five boom You have all the values in that list to begin with but sometimes you may not necessarily Have all the values you want at any given time or you may not want to To create them all at once. Maybe you want to create them only one at a time. And so what you can do is you can create a generator, and every time you call a generator, you get the next thing in sequence, right? So a good example of a generator would be something like Fibonacci numbers. What's the next Fibonacci? Well, go back to the previous two. Right, anyway. Or like reading a file line by line, right? You read this line, process it, then go on to the next one, read the next line, process it, and eventually it ends. What we can do is instead of returning this connection object for my database, I can yield it. Ooh, syntax highlighting. Yield is the keyword that turns any method into a generator. And what it does is it's going to return every call in sequence the next item. And so a fixture that is a generator might seem weird, but it actually means that there's two phases, right? The first time you call that fixture is the setup phase. The second time you call the fixture is the cleanup phase. And that's why we use generators for fixtures. Isn't that cool? That's so Pythonic, it's awesome. So then what I can do after my yield statement, I can say something like connection.close like this. And so my setup is everything before the yield, my teardown is everything after the yield. And so, and a lot of times what you find is that, okay, well, you could say setup and cleanup are two separate concerns, but honestly, they're really the same concern because usually setup and cleanup operate on the same thing. I never thought of it that way. That's why PyTest is awesome. So when it comes to this particular database object, this database connection that we're making, I am choosing to keep the setup and cleanup together. The same thing happens with other things, like if you're using Selenium WebDriver for your calls or if you've got some sort of API client you've got to connect to. Up here with my reading my database config, there's nothing really to tear down so I don't need a cleanup phase. But sometimes we do, and that's a that's a bit of a separate topic Can it can I solicit question any more remaining questions on fixtures before I move on to? Okay. Are there any other questions about fixtures before we talk about mocking? Yes. At the end of the test. Yes, yes. So the way that tests will operate is that you launch the pytest command, python-m pytest. Pytest will go to your project from whatever location you're on the directory. It will go from that point and discover all of the test cases. Once it has that, it has it basically as a giant list. portion, right, it'll do that version. Then it will run the body of the test function. And then afterwards, if there was anything left in the generator, meaning a tear down phase, then it will run that. And that will be all part of that one test case run. Gets the result of that, reports that out, moves on to the next test case, does the exact same thing and just knocks them down like dominoes. Yeah. Cool. Question back there. Heck yes. You can have multiple fixtures. I believe the order in which they would be called is the order in which you specify, though I don't know if that's guaranteed. I'd have to try it out, look up the docs and all. But yeah, you can have fixtures calling fixtures, and you can have fixtures or test cases calling multiple fixtures at once. So what I have found in larger test projects is that I will have many, many fixtures, right? There will be these kinds of general fixtures for things like read in my configuration information set this connection Start this web driver or start this API client create these objects, right? And even even within that there are going to be some fixtures that would only apply to a subset of the tests as well So they might be more localized,. Like in our accumulator class here, we created this accumulator object. These are really only applicable for these tests, so they should probably be localized here. There could be other, like you might need multiple different fixtures for, let's say, this accumulator object. Let's say we modified the accumulator class such that you could have a starting count Right where you can pass in an initial value Right and so if we were trying to test that well then maybe we would need to have enough a fixture for when it's at zero test, or should they only be reasons why you have to do that, but try to avoid it as much as possible. Yes. And I would never just be like, hey, some object out here, right? A constant I would do that, anything else, no. I would be using session scope fixtures or function scope fixtures or module scope fixtures or whatever kind of scope fixture that you need. Go read the docs. Very good. These are excellent questions, y'all. Any other questions about that before we move on? Oh my goodness, here we go. Yes. Mhm. What's the question? Okay. I'm not sure I'm following because I think I'm getting a little confused on what we're referring to because when you're saying steps, I'm thinking within one test case operations that would be performing sequentially, right? Here, I would not say these are steps. I would say these are distinct test cases. Yes. You're talking about within one test case going through multiple operations. Okay, okay, I see yeah because it if you're outlining a test procedure like that I mean there's inherent dependencies in between steps, right? surprise, right I Think you as the tester need to decide the length that. Or you just want to do one whole workflow, then, yeah, I mean, if something at step 205 explodes, okay. I mean, that's how you design the test. Testing isn't art as much as it is engineering. If you want to chat more, because if I'm not nailing something in your question, we can chat after the... Very good, very good. Okie dokie. So, someone had asked about mocking. I'd like to cover that briefly right now. There are times when in our unit testing that we can directly call things like this accumulator class and everything's hunky-dory because there are no external dependencies going on. But then there are other times where we may be calling out to something that has a dependency, right? Like, hey, maybe there's a module that's calling out to a database to load some data or something. It's a very common example. We already said before, for something to be truly a unit test, we don't want to have it make these calls to external dependencies like file systems or databases or APIs. And so how we can still unit test these things is by mocking them or faking them because there's a difference there. Has anybody done mocking, let's say, with like unit test mock or anything? What do you all think about that? Have you had a good time mocking? Or has been absolutely heckin terrible? It's been pretty bad right mocking sucks. Don't do it That might be a bit of a spicy take but but let me let me unpack this the way that mocking works is It's very hacky, right? It's easier to do in Python than other languages because Python, nothing's private, everything is changeable, you can just go in there and do very devious things. And so some of the things you can do in Python would be like, hey, you know, I have this object, why don't I just make like a mock of this object where I'm going to override essentially like all the different attributes and all the different method names of it. And I can set them to like return dummy values or to be dummy values. And I can pass that mock along such that when I have another method that happens to be using that object, well now it's using my mock object and it's making the dummy call. So I'm getting dummy data back so I'm not actually calling the database or the API or something. And so that you can kind of remove that dependency and you can give it a deterministic value, right? Like, oh, if this database call is supposed to return a certain kind of record, then you can just make a fake record to send back. There's also a technique called monkey patching, where it's like you change what a function does or a method does, and you're just like, hey, instead of calling this, call this instead. Same kind of principle, right? There are raw ways you can do this. I think most people would probably use unit tests mock. There's a PyTest fixture called mock that gives you unit tests mock that you can do devious things with. The problem with mocking like that is because it is hacky, anytime things change in your code, it's more likely to break things in your tests in ways you might not expect. It's also, I don't know, it just feels very dirty and complicated, right? Like, wah! You know, it feels very, very dangerous. And a lot of times I find that people who are inexperienced with this end up testing the wrong things or not testing the right things through or around these mocks that they've created. I think ultimately what is a better practice is not to use mocks that go in and hack things, but rather to design your code in a way that you can make fakes instead. Here's what I mean by that. Let's go with the database example we had before, where let's say that you have an API service that you're testing. You're trying to test the domain layer, and it's making a call to a database via SQL alchemy or something. And it's just SQL, right? Send SQL queries or ORM, right? Boom, boom, and back. If you were to mock that, you could like try to go in and like intercept where the connection is happening and sending these SQL queries. You could maybe do, if you're using SQL alchemy, go in there to the SQL alchemy library and start patching some of those methods and all. But rather, what I think the better practice would be is rather than have like your fast API routes directly make calls to SQLAlchemy, you should probably have a repository pattern in there, a capital R repository pattern, not a GitHub repository, I'm talking domain-driven design repository pattern, where basically you separate the concern of all that database stuff into the repository. There's an interface for it and specific methods for things like, hey, get this, get that. And inside this repository object that you create, that's where you're making your SQLAlchemy calls or whatever NoSQL calls or whatever kind of database connection calls you make. And what you can do, if you have an interface with a concrete repository for the real thing, when it comes time to testing, you can then use that same interface for your repository and make a fake database. And so all it is is just implementing that interface to fill out the same methods. It's kind of like a mock, but it's like a real thing. Because what's nice about this is that it's cleaner. You control it more, and you can inject it directly if you're using dependency injection. And so what I recommend people to do is think about designing your software in a way that you can have those kinds of fakes rather than mocks. I think that if you find yourself in a position where you want to test with mocks, that is a code smell, and you should be redesigning your system such that you can have fakes in its place. And so then you make the fake, and I've got test repositories where I've made a whole with And it's also more readable and understandable because if you can be like, oh, hey, so that's what, this is what a repository actually looks like. This is my adapter for how I make database calls versus hacky, mocky, what are you doing with this method here? Why are you adding these particular values in that way? And so that is what I strongly recommend. And I strongly recommend this so much that I explicitly do not teach how to use mock in this workshop. So if you're asking me, can you do an example of mock anyway? No. I stand on my principles. Sorry. If you want to do mocking, I would say pytestdocs, unitestdocs, and chatgbt. That'll get you through. But nevertheless, very good question. Cool. All right. We have more. Just like with any other coding assistance, Copilot, chat GPT blackbox AI you name them like if you already know Roughly how to do the thing and you just need some help sketching out the boilerplate It'll be great because it'll help expedite you if you if you don't really know how the thing works You you're going to end up in trouble. And so it's like, yeah, you can make chat GPT spit out a whole bunch of Pytest tests. Your mileage may vary. I mean, I've seen it pop out some Selenium WebDriver code. It's like, that's cute. All right. That's helpful for me, but heaven help somebody who's never done that before. Yeah, I mean, that's why I think it's more important to know not just the framework but also how to do proper testing. Because like, I don't know what ChatGPT would give you, but it might give you something where it's like one giant grand tour of 27 assertions in one thing and you're like, but this is covering multiple behaviors, is that right? Probably not. But if you didn't know better about good testing practice, you might be like, okay, I'll take it. That kind of thing. So it can be helpful if you know what you're doing. All righty, friends, let's move on. Let's get our hands back on the keyboard. All right. So, what we're gonna do next is we're not actually gonna write more tests, but we're going to start to play with the tests we already have. Because one of the really cool things about PyTest is that it has a rich command line to it, right? Python tends to be very command line friendly as it is. And so it can be really helpful to know a lot of the power of pytest's command line to help you do testing in the moments when you need it or when you need to do more specific things. For example, so far as we've been running these tests, we've just been running the whole test suite all at once, right? Right now we now have two test modules, but don't be surprised in a big test project that this could balloon to 200 test modules. It's not uncommon for test projects of notable size to have hundreds, thousands, even tens of thousands of different tests at all levels, unit integration end to end. And so we need ways to be able to say, hey, maybe I don't want to run every single test. Maybe I only want to run tests in a specific module. Or maybe I only want to run one very specific test that I pinpoint because I'm trying to reproduce some error, some failure. So if we wanted to do that, what we can do is we can provide the path to the test module or the test case that we want to test. So here I'm gonna say pytest tests. Let's say I only wanna run the accumulator tests. Let's provide the path to the module. Magic happens. Surprise. Hopefully nobody's too surprised. Or if I wanted to, I can provide an explicit test name. Hey, maybe I want to do this test. What you do is you give the path to the module, colon, colon. That's two colons. lots of colons. I only have one colon, but this thing has two. Test accum... Somebody's finally laughing at my lame jokes. Thank you. If I had a prize, I would give it to you. Test accumulator cannot set, boom. So you put the test name, ta-da, ran only one test. Now, if you're liking an IDE or something, like I could have come over here to whatever the runny thing is, additional views. Is there a tester view? Let's see. Oh, it's not configured for Python. Well, if I had it configured for Python, I could probably do it there too. But if you want to be command line driven, ta-da, you can do it like that. Other things that we can do It is often desired to to Perhaps run tests of a certain type or a certain name and So what I can do is you know hey, maybe instead of maybe I have multiple accumulator Tests across different modules and trying to concatenate like you know this module this module, this module, this module together. That can make a very long-winded command. You can also search by name matching on the test cases. So python dash m pytest. Let's say I wanted to match them. I wanted to run any tests that have the name accumulator in there. Use the dash K option and it'll find all the tests that have that name in the function name. So here you can see five tests passed, 13 deselected because the 13 math tests did not have the name accumulator in them, right? So you can filter like that. You can even use basic Boolean logic with that kind of command. So I could say, what if I want to run, let's say, test with one, right? That actually has tests from both accumulator and math, but let's say I wanted to run the ones that said one and not accumulator, meaning I only want to get the math functions that ran that. And so there, boom, it only ran it from, the two from the math, right? And so basic Boolean logic here, just like you'd see in Python, and or not supported. Yes? Is it doing any word guarding around the patterns there? Could one be a part of a larger word by accident? I mean, it would be substring. So it's just substring. Yeah. So the underscores in these names mean nothing. This is literally just string matching. The underscores are rather conventional. Python, snake case, love it. Take that camel case. I like to use these separators for snake case in my test names just to kind of make them more readable. Also, this is one of those cases where longer names are acceptable, right? A lot of times we're like, ooh, long names in Python? No. When it comes to naming your test cases, the long names are helpful because they give more context, right? I want to be able to look at my test case name and know exactly why it failed when it fails, right? So if I see test accumulator cannot set count directly and I see this has failed, why do you think this particular test case failed? Well, obviously, I was able to set the count directly, and that's a problem. Readability counts. Now, this dash K option is rather dangerous, right? Because, yeah, it's really cool that I can filter by string matching of all the things that I discover. But there's probably going to be some false positives along the way in a very big project. I don't like that. I don't want to run things I don't have to because testing is very expensive, right? Unit tests are very quick, but integration end-to-end tests, that can be very painful. Is there a way that we could tag our tests so that we could say, hey, I want you to run these versus those? And the answer is yes, we can tag our tests. And the way that we tag our tests is with another PyTest decorator. And so what we want to do, we already have imported PyTest, I'm going to do this in my test math module pytest mark we're going to mark our tests now this might actually look familiar because if we scroll down and we see the parameterized thing Pytest mark parameterized, so we've actually marked things before what I'm going to do now is I'm going to create a custom mark pytest mark mathMath. Boom, boom, boom, boom. Let me add this to all the test cases. Yes, you have to do it for each one. No, there's not a way that you can just add it in one place. Sorry. Why? Because that's the way it works. I didn't make it. Go complete a PyTest, people. Yes. And you can actually have multiple markers for one test case. So if you wanted multiple tags or whatever, you could. And then to round this out, come over here. Let's give these tests a different name or, excuse me, a different mark. Let's call these the accumulator tests. Oopsie. And I will add, oh, wait, that's to the fixture. No, no, no, don't put it on the fixture. That's wrong, that's wrong. There we go. I almost did a bad thing. I make mistakes too. It's okay. I math for all my math tests. Now, this might not be super valuable in our small project case because, like, well, all the math tests are in this math module, and same for accumulator. But if you had, you know, across multiple modules, right, or if you had mixed tests within one module, right, like, I could mark each of these with the math operation that they perform, you know, and that could be another way to mark these things. So we've added our decorator. There's one more thing that we're going to have to do. If you were to run this right now, PyTest wouldn't be too happy because it would be like, what the heck is this mark? I don't know what that is. Why didn't you tell me? Explicit is better than implicit, and you left me in implicit land. I don't like that. So we've got to make it explicit. And to make it explicit, what we've got to do is we've got to configure pytest for that. So we need to add a pytest.ini file for configuration. In the root level of your project, new file, pytest.ini. There's a couple different formats you can use for pytest configuration files. I'm going to use pytest-ini because I'm going to use pytest-ini. What we do in this ini file, we give it a pytest section in square brackets and we say markers, just like we were kids, equals, and then underneath of this we can provide the list of markers, accum and math. They don't't need to be alphabetical I recommend you do alphabetical. Otherwise, you won't ever find them again. Very sad Cool, and so now I have declared what my markers are I have applied them to my test cases You can add any markers you want. It's cool. Make sure they're just like a single token and now when I run my tests what I can do is instead of using this wildcard-k option, which is rather dangerous, I can say dash m for mark, and I can provide the mark name. Math. Boom. And again, you see we've got the ones that were in there. We've got the ones that were deselected. I can also do a Qoom. And if I had these across multiple modules, that would be elicited as well. And so this is the way in which you can filter and search and tag your PyTest test cases. What might be more interesting would be things like where's my PyTest.ini file? Things like units, integration, E2E or something, or maybe we could do like API web, I don't know, stuff like this. And then I could come back here, my accumulator tests, these are all my unit tests. So I could still call them accumulator, but then I would also call them unit, right? Typically, it's a good practice to run my different kinds of tests in different phases. Hence, I would run unit tests separately from integration tests separately from end-to-end tests, right? So now I could be like, hey, look at this, unit tests go, woo, now we're doing recommended practices. Booyah Yes, I Don't know let's find out maybe maybe not Unit and a cume that works. What about unit and math Oh unit yeah and What's special about the parameterized mark, like so. It's honestly just saying it's marked with these parameters. I don't know why they didn't just do pytest parameterize. I don't know why they did pytest mark parameterize and it's still on the error? I guess I wouldn't see that as being very useful. I think that's more like a interesting side effect than anything Huh? What do you know? I'm learning stuff today, too. Are we learning a lot today? Yeah, that's good. That's why you came Cool. All right. Shall we move on? Yeah, we only got half an hour left oh my gosh, we still have more to cover we haven't even gotten to box testing. I don't know if we're going to get there. That's okay if we don't because you can get all the stuff online. All right. In addition to filtering your tests, there are a bunch of helpful command line options for running in different ways. For example, if you don't know what to do with pytest, any of the commands, anything like that, you can always run the help command. Look at this. So awesome. Now that we have chat-jpt, I don't know if we need this anymore, but oh my gosh, look at all these options. You can go get help on the manual. Maybe you should grep that on the way. I don't know. But hey, look right here, K, M. We're going to go through all of them. No, we're not. But yeah, it's overwhelming. When I said this was a very powerful command line, I was not kidding. And then there's also the configuration file options, pytest ini options. They're all spelled out here too. Most of the command line options have a configuration file equivalent and vice versa. I don't know if it's perfectly one-to-one, but like for example, like the path, you can actually put the path into, like we were doing the paths of testing, like saying, you know, give you everything, maybe under the tests directory, you can put that in the INI file too. Stuff like that. Okay. So, let's explore some of these options, what they do. One of them is the exit first option. So, if we run our tests, Python dash test as we know it runs them all and if I were to inject a failure Let me go get my math tests open again if I were to inject a failure whoa I can't see If I were to inject a failure, let's do it. Let's do it here again. Now, I inject the failure. Pytest will still run all the tests, right? Even despite this. But there may be times where we want to halt it and be like, hey, once I hit a failure, I want to stop. If we want to do that, I can add the dash dash exit first option. And then as soon as it hits that failure, look what happens. It stops and it dies. So that way, oh shoot, I hit a failure, don't run more. That might be useful to you. I don't think this option is very useful, to be honest. What I prefer, instead of the exit first or simply dash X option I prefer the dash dash max fail option. I think somebody was asking about this earlier. Yeah You were ahead of the game Max fail, you know, let's say I want to Fail only after let's say ten tests fail, right? And so here that that was now a tolerance level where I had one failure. that's okay, I can keep going. But if I hit that magic number 10, at that point, I'm gonna stop. Typically, what I recommend when, I don't recommend halting for unit tests, because your unit tests should be so quick, it doesn't matter. But in your black box tests, I would recommend halting if you hit a certain number of tests that would save a significant chunk of time. Let's say you've got a suite of a thousand web UI tests, and it's going to take three hours to run this thing. If you've got a hundred tests that are failing, you probably should stop that test suite as soon as you hit that magic number of like 50 or a hundred, let's say, because there's no point in running it for the full three to four hours if it's just gonna be more and more and more failure. You have enough to triage, right? That's kind of the magic thing. When you have enough failure to be worth triaging, that's the point at which you should say, hey, stop, don't run more. Remember, compute resources are not free, whether you pay them in the cloud or you pay for them on your own premises. So no need to do wasteful testing. All right, so that's the max fail option another option you can do I mentioned this before as well Somebody asked about reporting is like hey What if I want to get some test reports other than what's puked out on the screen here in the terminal? And one of the most common report formats for better or worse more for worse worse than better, is JUnit XML. Pretty much every single CI tool that runs tests accepts JUnit XML format. Why? Because Java was dominant for so long, and JUnit is the Java test framework. If you haven't used it before, basically it's an XML report that shows pass-fail of all your tests. It's nothing magical. But PyTest and almost every test framework in every language has some sort of output option that lets you get a JUnit XML file. And so to do it in PyTest is very simple. Dash dash JUnit dash XML report XML. You can give the file name. And so here now in my root level project, I now have this report XML file and if you look at what's inside is nothing exciting right? It's an XML file. And it actually includes that introspection for your assertion, which is nice, but yeah, you probably shouldn't be reading this as a human. Just give it to Jenkins or Travis or whoever is doing your CI now. Azure DevOps. I don't know. Do people still use Jenkins? Is that a thing? Yeah, lots of people. Okay. I figured as much. At my company, we use Azure DevOps. Anybody use Azure DevOps? Yeah? Not as many. Sorry, just you and me. Oh, okay. Some more people. Okay. Cool, cool. Right, right. So anyway, that's how you can get your GUnit XML. Some other things that you can do in terms of running the tests. Oopsie. Oh my gosh. Straight from the command line. When we would run it before, we get the standard kind of output that we expect. Sometimes you might want more information on the command line. And so you can do dash dash verbose or dash v verbose. And when you do verbose, guess what? It's a lot more verbose than the output it shows. Rather than showing each module and then dots and f's, it will actually spell out every single test by name and explicitly say it was passed or failed or whatever. And then here you can see the progression of execution with that percentage on the side. What's really cool about this is if we come down to the multiplication test and that absolute value test, remember how we had the parameters for it? It'll actually print out the different parameters used. Yay. Why might you ask are there double minus signs in there? Because these were negative numbers. All right. And then, yep, you see everything there. And then if we were to fail, just to see what that looks like with verbose option. Again, it's still the same banner. You just get more going up here. And you can see, oh, it turned to red, very sad. What if you want the opposite of verbose? What is the opposite of verbose? Quiet. Not quit, quiet. What's gonna happen here? Very minimal output, right? All it does is it printed the dots and Fs and gave you a very short little message of how many passed and failed within the time limit. Why would quiet be nice? I don't know. Maybe you're sick of way too much puke in your CI pipelines in the output, maybe, because you already have that JUnit XML. It's going to suck up anyway. Maybe it's that you're just trying to bleed through something very quickly, like repeatedly debugging, trying it, and you don't want all that ugly stuff coming in your face. I don't know. I don't really use quiet, but I show it to people in case they want to use it. So those are typically the command line options that I've used or I've seen people use. There's plenty, plenty more you could get into. The only final thing I would show with command line options is how you can add it to the INI file. So, there are some times that there are certain options you always want to include. Maybe it's like a team level thing where, hey, we as a team have decided upon these standards, and so we're just going to include this like here. And so to put it in a fixed place, like an INI file to be read, rather than having to remember it every single time you run the command on the command line, could be very helpful. So if you want to add any command line options in your INI file, you say add ops, ops, A-B, oh wow, A-D-D-O-P-T-S, and then you can add all the command line options you want so let's say I want to always run verbose Something like this now when I whoopsie when I run my tests notice how I'm not including any options Boom it's now running in verbose Tada happy I don't think I want that though. Well, you know what, can I? Oh, that's allowed. Okay, cool. So I'll comment that out for you. But yeah, so you can set your options there. Cool. Is good? Any questions about all that? Question? Oh yeah, that sounds familiar. Notice how it can know that context. Notice, well, where's, there's this PyCache, right? I think there's like a PyTest cache somewhere too, usually. Here we go. This is what I'm looking for, PyTest cache. So it's storing some things on your machine as well. Don't commit that to Git. Make sure you have that to Git ignore, please. Cool. So we feel pretty confident about this pytest stuff. Oh, question. Yeah, yeah, yeah. So you could do it this way. You could say quiet and then JUnit XML report.x, like this. And so it'll be quiet on the terminal, but still give you your JUnit XML. Right, so ta-da! Woo! Right. Magic. All right. Cool, cool. I've been talking way too much. All right. So we have 20 minutes left. I want to talk about plugins next, if that's cool. As I had said before, plugins are one of those magic things about PyTest that just make it phenomenal. Fixtures being one, plugins being another. There's a couple other things too. But the idea of a plugin with PyTest is to say, hey, the core framework that we've been playing with is another but that's kind of the onus is on you for that. But it's, the test framework is fairly simple. It provides the structure for writing tests, and when it goes to run, it discovers all the test cases, it lines them up like dominoes, and it knocks them down. And then it gives you the pass fail result for each one. And that's all a test framework really needs to do. Anything else are the bells and whistles on top. And so if you want PyTest to do more for you, you can extend it with plugins. There are a myriad of plugins available out there. If you go to PIP or PyPi, you can see pretty much the naming convention for plugins is usually PyTest dash whatever. And so you can go install plugins that other people have created. You can also create your own plugins. You can create your own plugins. Essentially, you making fixtures in a comptest.py file is making a plugin. So congrats, we already made our own plugin. We just didn't realize it. Plugins usually are their fixtures or they're going into some of the hooks behind the scenes. If you make your own plugin and you want to release it back, you can make a Python package and push that out. Next thing you know, you are now a open source contributor and Python package author. We're not going to go that far today, but what we will do is we'll cover some of the most popular plugins that I have seen and used. Okay, so first plugin we're gonna look at is one for reporting. I know reporting has come up a couple times. We've seen the report XML file here for JUnit XML. This is not human readable. This is meant to be machine readable. What if we wanted a more human readable report? Well, there is a plugin out there called Pytest-html. So on your command line, in your virtual environment, pip install pytesthtml. There it goes. And now what we can do when we run our tests, we can provide the dash dash HTML option and say, give it a path, report.html. And what this will do when it runs our tests, it will generate an HTML report for them. But nice. And so here you can see the banner and stuff looks almost identical. You will see here now, though, that there is a plugins output that shows, hey, I'm using the HTML 411 plugin. And also here, it added output to that banner to say, hey, here's your report file. Here's the path to it. It's going to be in my local directory because that's just where I plopped it. And now if I open that, look, I have an HTML file. That's cool. If I were to open this in a web browser, oh, it came up on the wrong screen. Here, let me scooch this over. Ta-da. Look at that. That's a little tiny. Let me see if I can... Ta-da! Very, very basic. It's not fancy, but it shows you, hey, look at all these things. And you can click them, and it captures any outputs, right? Very, very rudimentary. Can you customize this? Yes and no. Out of the box, as it's given to you, like, I mean, it is what it is. You want to customize it? Guess what? It's open source. Have fun. You don't like this report? Make a better one. I'm not trying to be coy. It's just that's, this is meant to be a basic kind of, you know, HTML. Like seriously, if you don't like this, read the code, rip it open. You could probably easily make a better page than this. I don't know if I could. I'm not that great with CSS and web design. But I mean, not to knock on this, but it's like, meh, right? But yes, you can make your own reporting like that. So that's the Pytest HTML plugin. There's another plugin I want to share next. Or any questions about that plugin before I move on? Yes? What do you mean? Yes, so if you are testing performance Performance testing is fundamentally different from functional testing Pi test is not a great framework for performance testing with functional testing. We are we are Testing to verify if something meets expectation with the binary pass-fail result. With performance testing, you're trying to see how well something performs within a certain metric. And so with performance testing, the way you typically want to set that up is to say, let me run the thing, capture the metrics, and look to see how it's doing. You could probably make a plug-in, if there isn't already one out there there that says, if tests run more than X number of seconds, kill it and market has failed. You probably could do that. I probably wouldn't recommend it. Because those are two separate kinds of things I'm looking for. There's one thing to say, hey, does the test actually work? Or does the behavior actually work or not versus how well does it perform? Performance is very dependent upon the environment in which it's run. And so it's like there might be, because of reasons, there might be slow days where it's like, well, you know, if you were to do something like you were suggesting, if you're on one of those slow days, it's like, well, I can't get the feedback I need on the functionality because my test keeps cutting off. But if I just gave it a little more time, it could at least tell me whether or not the functionality is correct. I know I have the performance issue. Don't block me on the functional issue. That's how I would handle that. Very good, insightful question. Any others about reporting? Cool. All right. So next plug-in I want to show is on code coverage. Somebody in the room mentioned code coverage before. In Python, the way that we do code coverage for most of us is coverage.py. I'm sure many of you have probably heard of or used coverage.py. Maybe, maybe not. It's a pretty cool module. It does what's needed. There is a pytest plugin that brings coverage.py automatically into pytest, which is beautiful. We don't really have to do anything to set things up. We just have to install this plugin. So here this time we're gonna do pip install pytest-cov, C-O-V. Bam. And there we go. Boom, we got this one all right pi test dash cove and Now when we run it What we do is we say okay? Oops? Where's my? There's my command We have to give the dash dash cove option and We give the path or the paths To the directories that we want to cover with code coverage So in this case, which directory do you think we should cover with code coverage? Stuff that's right Because that is where our product code is should we cover the tests directory with code coverage? No, why not? That's right, that's not the product code. That's not what we're trying to test. They're going to be covered anyway. What we care about is the stuff directory. All right, so when we run this command, I'll increase the terminal again. So now we can see we've got the coverage plug-in in there, again, pytest-cov. We see the printout of all the tests running, and then we also see this happy little table. Look at that. So we had all this stuff covered. Yay, we're fully covered. Wonderful. Now, we also ought to... This is just puked out on the command line. We can also generate an HTML report or other kinds of reports. To do that, what we can do is we say dash dash COV dash reports and then give it the report format we want. In this case, HTML. There's a couple different report formats. I can't remember all of them. But if we do HTML, we can see here now, hey, HTML, I've got, oh my gosh, we have a whole static site. So if I were to open up this reveal this in finer, open it up. Now we have a web report for our code coverage. Right, and I can go in here, look, these are all the statements and stuff. Right, everything is covered, everything is hunky dory. Final thing I would say about code coverage, oh my gosh, where's the thing? Final thing I would say about code coverage is by default it does statement coverage, not branching coverage. That's a big no-no for anybody who's in the know. So, what I would say is make sure that you are always doing dash dash COV dash branch. If you know what I'm talking about, you'll be like, ah, yes. If you don't know what I'm talking about, just always add dash dash COV branch. Trust me on it. It will make things better. There are times where certain if else conditions may not be appropriately covered if you don't do branch coverage. Okey dokey. All good with code coverage? All right, the last plugin I'm gonna show you today is for running in parallel. Yes, somebody asked about that before too. Now we're gonna actually see it in action. So far we have been running our tests serially, meaning one at a time. For unit testing, that really is not a big deal, because unit tests should be very quick, no dependencies, all in memory. But when it comes to black box tests, API test, UI test, Milver test, whatever else, those tests take a lot more time typically, right? If we look at the rule of ones, order of magnitudes, unit tests take about one millisecond. API tests take about one second because they got to hop the network and get back and do that kind of thing. So about one second per API test. UI tests take about one minute. Why? They got to either open up the browser or open up the mobile app. You got to wait for the things to load, navigate, wait, click, wait, wait, wait, click again, wait, close it all down, right? So order of magnitude, those tests take about a minute. Rule of ones, millisecond for unit, second for API, minute for UI. So if you do some quick math, how long would it take you to run, I don't know, like a thousand tests of unit tests? One second, order of magnitude, not a big deal. How about API tests? A thousand tests would be a thousand seconds. That's, what is that? A couple of minutes. You know, not too bad. What about a thousand UI tests? Oh, shoot. That's going to be all day. That's going to be two business days. It's like 16, 17 hours, right? Oof. And let me tell you what, friends, a thousand tests is not a large number of tests. That is a fairly standard number of tests. There's a whole pyramid we talk about this. I'm not going to go down that path. But what I am going to say is that when you run your tests serially for black box testing, you burn a lot of time. One of the most effective ways to reduce that total time you spend testing is to run them in parallel. Now, what that means is that your tests need to be independent of each other so that you can run them in parallel. You can run them in any order. Things happening in the system won't collide. And there is an art to that because data is always going to be your nightmare. But assuming that you have tests truly independent, you can parallelize them to any scale. And so if we can start to run our tests maybe three times parallel scale, five times parallel scale, 12 times parallel scale, even up to like 50, 100 times parallel, we can take that huge amount of time and drastically reduce it. So parallel testing is one of those amazing ways to do more testing in less time. You will need to, like I say, construct your suites right, and you will need to give it the firepower. You're not going to run 100 web tests in parallel on this machine. That's not happening. You need to scale out across multiple machines to be able to do something like that. But thankfully with PyTest, guess what? We can do it. Need a new plugin though, pip install pytest-xdist. That is X-D-I-S-T for extreme distribution. I don't know if that's really why it's named, but that's why I always think it's that way, because you're distributing your tests, right? Go ahead and install this bad boy. Clear. There we go. So now, if we want to run tests in parallel, we run our command, python dash m pytest dash n, n for number of workers, meaning number of parallel threads. I don't know, let's give it three. Boom, look at that. So here we can see now we had three workers. Because they're running in parallel, it's kind of hard to print out the whole what module's running at what time because now it's all jumbled, but that's okay. I could give it as many workers as I want. Doesn't mean it's going to be healthy. Maybe I could do 12. I'm on an M1 Mac, right? There's a point of diminishing return because it's just not worth it, right? But if I go back to what was that, three before? So that was a quarter of a second. How long did it take to, I'm just curious. Okay, one second. There is overhead to parallel execution, so you don't really need to do parallel for unit tests. But if this were like API or UI tests, it would make a big difference. There are also some special options with PyTest Xdist, I think, about like multiple machines, if I recall correctly. I can't remember, though. But anyway, if you want to do parallel, it's just pytest x dist and then that extra little option there question It should oh, yeah, it should so let's let's validate that what was the HTML thing again, I can't remember. Yeah, HTML equals report.html. It should. I don't see why it wouldn't. Report.html. Ta-da. Look at that. Magic. The framework's working for you. good question Pre-commit yamil stuff Hmm I don't know about pre-commit, but definitely for pull requests. Every time you open up a PR, it should run your unit tests and give you code coverage. Feature level tests, that's debatable because there's a whole much more with that. But unit tests and code coverage on every PR, absolutely. Absolutely. Cool. Very good. So I know we are fast out of time. Before I solicit more questions, I want to share just a wee bit more information. There's more to cover regarding pytest and good testing and all that stuff than I could possibly cover in one day, let alone an afternoon. So if you go back to that repository I shared before, there are examples of how you do API testing and UI testing. If we look at the API, oh, that's really tiny let me If you look at API testing I mean honestly what I recommend for most people is just import requests and go to town right Or your preferred HTTP client in Python you know here's an example test where I'm calling the duck duck go instant answer API We're We're doing an ArrangeAct assert. ArrangeAct asserts the way you write good tests. You set something up, you exercise the behavior, and you verify correctness. So here I'm setting the URL, searching for Python programming. You know, request dot, whoa, that was unexpected. Anyway, that's all good. Requests.get for my URL, parse the JSON response body, and then make sure you're checking things like status code and values in your response. Very, very basic API test. How do I go back to? So then if we do my UI test, here would be an example of a web UI test with playwrights. You can check this out too. So we're loading a page, we're logging into it, and then we're making sure that certain things appear on the page as we expect. So we did not have time to go into these deeper, but definitely check that out on your own. There's also on Test Automation University, like I mentioned before, the Playwright Testing Path. Much of what we covered today was in the Introduction to PyTest course. Look, it's me. I made this course. Yay. We basically covered chapters one through nine. You can do chapters 10, 11, and 12 as homework on your own. If you use Google Test Automation University, there's only one PyTest course out there. So check that out. Hopefully that can kind of fill in what else there is to know about PyTest. If you want to go even further with your Python testing, again, there's more courses in that learning path. You can learn about how to do web element locators for web tests. Python stuff, behavior and development, Gherkin, given one then. There's a course on that too, in Python. So that being said, are there any final classes before we go to the opening reception? Very good. Well, thank you all for lasting to the end. I know a three and a half hour tutorial is a very long time, especially if you did one in the morning. Again, my name is Andy Knight. I am the Automation Panda. I will be around tomorrow until about three or four o'clock, then I have to go home. But yeah, feel free to reach out to me. Connect with me on Twitter at Automation Panda. Connect with me on LinkedIn. If you just Google this moniker, you will find me. Thank you.

